Working Directory: /home/s_felix/mdcath-processor

File Structure:
.
├── AI_context.sh
├── AI_context.txt
├── check_environment.py
├── LICENSE
├── main.py
├── mdcath_processing.log
├── msms_executables
│   ├── 1crn.pdb
│   ├── 1crn.xyzr
│   ├── 1crn.xyzrn
│   ├── atmtypenumbers
│   ├── msms.1
│   ├── msms.html
│   ├── msms_i86_64Linux2_2.6.1.tar.gz
│   ├── msms.x86_64Linux2.2.6.1
│   ├── msms.x86_64Linux2.2.6.1.staticgcc
│   ├── pdb_to_xyzr
│   ├── pdb_to_xyzrn
│   ├── README
│   ├── ReleaseNotes
│   ├── test.pdb
│   └── test.xyzr
├── outputs
├── README.md
├── requirements.txt
├── setup.py
├── setup.sh
├── src
│   ├── mdcath
│   │   ├── config
│   │   │   ├── default_config.yaml
│   │   │   └── __init__.py
│   │   ├── core
│   │   │   ├── data_loader.py
│   │   │   ├── __init__.py
│   │   │   └── __pycache__
│   │   │       ├── data_loader.cpython-39.pyc
│   │   │       └── __init__.cpython-39.pyc
│   │   ├── __init__.py
│   │   ├── processing
│   │   │   ├── core_exterior.py
│   │   │   ├── features.py
│   │   │   ├── __init__.py
│   │   │   ├── pdb.py
│   │   │   ├── __pycache__
│   │   │   │   ├── core_exterior.cpython-39.pyc
│   │   │   │   ├── features.cpython-39.pyc
│   │   │   │   ├── __init__.cpython-39.pyc
│   │   │   │   ├── pdb.cpython-39.pyc
│   │   │   │   ├── rmsf.cpython-39.pyc
│   │   │   │   ├── visualization.cpython-39.pyc
│   │   │   │   └── voxelizer.cpython-39.pyc
│   │   │   ├── rmsf.py
│   │   │   ├── visualization.py
│   │   │   └── voxelizer.py
│   │   └── __pycache__
│   │       └── __init__.cpython-39.pyc
│   └── mdcath.egg-info
│       ├── dependency_links.txt
│       ├── PKG-INFO
│       ├── requires.txt
│       ├── SOURCES.txt
│       └── top_level.txt
└── test_h5_loading.py

11 directories, 53 files

Contents of Relevant Files Below (Ignoring Binary Files):
---------------------------------------------------------
===== FILE: src/mdcath/config/default_config.yaml =====
input:
  mdcath_folder: "/mnt/datasets/MD_CATH/data"  # Path to the mdCATH folder
  domain_ids: [
    "12asA00",
    "153lA00",
    "16pkA02",
    "1a02F00",
    "1a05A00",
    "1a0aA00",
    "1a0hA01",
]

temperatures: [320, 348, 379, 413, 450]
num_replicas: 5  # Number of replicas to process per temperature

output:
  base_dir: "./outputs"

processing:
  frame_selection:
    method: "rmsd"  # Options: regular, rmsd, gyration, random
    num_frames: 4   # Number of frames to extract per domain/temperature
    cluster_method: "kmeans"  # For RMSD-based selection

  pdb_cleaning:
    replace_chain_0_with_A: true
    fix_atom_numbering: true
    correct_unusual_residue_names: true
    add_cryst1_record: true  # Add CRYST1 record for MSMS compatibility
    remove_hydrogens: false  # Whether to remove hydrogen atoms
    remove_solvent_ions: true   # If set to true, skip TIP, HOH, SOD, CLA, chain 'W'
    stop_after_ter: true #Stop cleaning after the first TER (this is only if later you wanted to use solvent ions for any reason)


  ml_feature_extraction:
    min_residues_per_domain: 0
    max_residues_per_domain: 50000
    normalize_features: true
    include_secondary_structure: true
    include_core_exterior: true
    include_dssp: true  # Extract and include per-residue DSSP data

  core_exterior:
    method: "msms"  # Options: msms, biopython, fallback
    msms_executable_dir: "./msms_executables"  # Path to MSMS executables
    ses_threshold: 1.0  # Threshold for classifying residues (Å²)
    sasa_threshold: 20.0  # Threshold for Biopython SASA (Å²)

  voxelization:
    frame_edge_length: 12.0  # Physical size of the voxel grid (Å)
    voxels_per_side: 21  # Number of voxels along each dimension
    atom_encoder: "CNOCBCA"  # Atom types to include (options: CNO, CNOCB, CNOCBCA)
    encode_cb: true  # Whether to include CB atoms
    compression_gzip: true  # Whether to compress the output files
    voxelise_all_states: false  # Whether to voxelize all states in NMR structures
    process_frames: false  # Whether to also voxelize frame directories
    process_temps: [320, 348, 379, 413, 450]  # Temperatures to process for frame voxelization

performance:
  num_cores: 0  # 0 means auto-detect (use max available cores - 2)
  batch_size: 100
  memory_limit_gb: 0  # 0 means no limit
  use_gpu: true  # Whether to use GPU acceleration if available

logging:
  verbose: true
  level: "INFO"
  console_level: "INFO"
  file_level: "DEBUG"
  show_progress_bars: true
===== FILE: src/mdcath/config/__init__.py =====
"""
Configuration handling for mdCATH
"""

===== FILE: src/mdcath/core/data_loader.py =====


#!/usr/bin/env python3
"""
Core functionality for loading and processing H5 data from mdCATH dataset.
"""

import os
import h5py
import logging
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Union, Any
from concurrent.futures import ProcessPoolExecutor, as_completed

class H5DataLoader:
    """
    Class for efficiently loading and extracting data from mdCATH H5 files.
    Uses chunking/streaming to handle large files.
    """

    def __init__(self, h5_path: str, config: Dict[str, Any]):
        """
        Initialize the H5 data loader.

        Args:
            h5_path: Path to H5 file
            config: Configuration dictionary
        """
        self.h5_path = h5_path
        self.config = config
        self.domain_id = os.path.basename(h5_path).replace("mdcath_dataset_", "").replace(".h5", "")
        valid = self._validate_h5()
        if valid:
            logging.info(f"Successfully validated H5 file for domain {self.domain_id}")
        else:
            logging.error(f"H5 file validation failed for domain {self.domain_id}")

    def _validate_h5(self) -> bool:
        """
        Validate that the H5 file has the expected structure.
        Enhanced with detailed logging and verification.
        
        Returns:
            Boolean indicating if the file is valid
        """
        try:
            with h5py.File(self.h5_path, 'r') as f:
                # Check if domain exists
                if self.domain_id not in f:
                    logging.error(f"Domain {self.domain_id} not found in {self.h5_path}")
                    return False
                
                # Log domain attributes
                domain_attrs = f[self.domain_id].attrs
                logging.info(f"Domain: {self.domain_id}")
                logging.info(f"Number of chains: {domain_attrs.get('numChains', 'N/A')}")
                logging.info(f"Number of protein atoms: {domain_attrs.get('numProteinAtoms', 'N/A')}")
                logging.info(f"Number of residues: {domain_attrs.get('numResidues', 'N/A')}")
                
                # Log atomic number information if available
                if 'z' in f[self.domain_id]:
                    z_data = f[self.domain_id]['z']
                    logging.info(f"z.shape: {z_data.shape}")
                    logging.info(f"First 10 z values: {z_data[:10]}")
                
                # Verify CA count matches numResidues if PDB data is available
                if 'pdbProteinAtoms' in f[self.domain_id]:
                    try:
                        pdb_data = f[self.domain_id]['pdbProteinAtoms'][()].decode('utf-8').split('\n')
                        ca_count = sum(1 for line in pdb_data if line.startswith('ATOM') and ' CA ' in line)
                        logging.info(f"Number of CA atoms in PDB: {ca_count}")
                        
                        if 'numResidues' in domain_attrs and ca_count != domain_attrs['numResidues']:
                            logging.warning(f"CA count ({ca_count}) does not match numResidues "
                                        f"({domain_attrs['numResidues']})")
                    except Exception as e:
                        logging.warning(f"Could not verify CA atom count: {e}")
                
                # Check for required metadata fields
                required_metadata = ["resid", "resname"]
                for field in required_metadata:
                    if field not in f[self.domain_id]:
                        logging.error(f"Required metadata field '{field}' not found for domain {self.domain_id}")
                        return False
                
                # Check for required temperature groups
                temps = [str(t) for t in self.config.get("temperatures", [320, 348, 379, 413, 450])]
                num_replicas = self.config.get("num_replicas", 5)
                
                temp_found = False
                for temp in temps:
                    if temp in f[self.domain_id]:
                        temp_found = True
                        logging.info(f"Found temperature group: {temp}K")
                        # Check for replica groups
                        replicas_found = 0
                        for r in range(num_replicas):
                            replica = str(r)
                            if replica in f[self.domain_id][temp]:
                                replicas_found += 1
                                # Check for specific datasets
                                # Removed DSSP from required datasets
                                required_datasets = ['rmsf', 'coords']
                                datasets_found = []
                                for dataset in required_datasets:
                                    if dataset in f[self.domain_id][temp][replica]:
                                        datasets_found.append(dataset)
                                    else:
                                        logging.warning(f"Dataset {dataset} not found for temperature {temp}, " 
                                                    f"replica {replica} in domain {self.domain_id}")
                                
                                # Also check for gyrationRadius and rmsd for frame selection
                                optional_datasets = ['gyrationRadius', 'rmsd']
                                for dataset in optional_datasets:
                                    if dataset in f[self.domain_id][temp][replica]:
                                        datasets_found.append(dataset)
                                
                                logging.info(f"Replica {replica} has datasets: {', '.join(datasets_found)}")
                            else:
                                logging.warning(f"Replica {replica} not found for temperature {temp} in domain {self.domain_id}")
                        
                        logging.info(f"Found {replicas_found}/{num_replicas} replicas for temperature {temp}K")
                    else:
                        logging.warning(f"Temperature {temp} not found for domain {self.domain_id}")
                
                if not temp_found:
                    logging.error(f"No valid temperature groups found for domain {self.domain_id}")
                    return False
                    
                return True
        except Exception as e:
            logging.error(f"Failed to validate H5 file {self.h5_path}: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return False

    def extract_rmsf(self, temperature: str, replica: str) -> Optional[pd.DataFrame]:
        """
        Extract RMSF data for a specific temperature and replica.
        RMSF is per-residue, so we build a unique residue-level list
        from the full 'resid'/'resname' arrays (which may be per-atom).
        
        Args:
            temperature: Temperature (e.g., "320")
            replica: Replica (e.g., "0")
        
        Returns:
            DataFrame with columns: [domain_id, resid, resname, rmsf_{temperature}]
            or None if extraction fails
        """
        try:
            with h5py.File(self.h5_path, 'r') as f:
                # Check if temperature and replica exist
                if (temperature not in f[self.domain_id]) or (replica not in f[self.domain_id][temperature]):
                    logging.warning(f"Temperature {temperature} or replica {replica} not found for domain {self.domain_id}")
                    return None
                
                if 'rmsf' not in f[self.domain_id][temperature][replica]:
                    logging.warning(f"RMSF data not found for domain {self.domain_id}, temperature {temperature}, replica {replica}")
                    return None
                
                # RMSF data is typically length = number_of_residues
                rmsf_data = f[self.domain_id][temperature][replica]['rmsf'][:]
                logging.info(f"Extracted RMSF data with shape {rmsf_data.shape} for domain {self.domain_id}, "
                            f"temperature {temperature}, replica {replica}")

                # Extract the full, per-atom arrays
                resids_all = f[self.domain_id]['resid'][:]
                resnames_all = f[self.domain_id]['resname'][:]

                # Convert bytes -> string if needed
                resnames_all = [
                    rn.decode("utf-8") if isinstance(rn, bytes) else str(rn)
                    for rn in resnames_all
                ]

                # Build unique residue-level list
                # Map resid -> resname (the first occurrence of that resid)
                # This ensures one row per residue
                residue_dict = {}
                for i, resid_val in enumerate(resids_all):
                    if resid_val not in residue_dict:
                        residue_dict[resid_val] = resnames_all[i]

                unique_resids = sorted(residue_dict.keys())
                unique_resnames = [residue_dict[rid] for rid in unique_resids]
                
                logging.info(f"Found {len(unique_resids)} unique residues for domain {self.domain_id}")

                # Check dimension mismatch
                if len(unique_resids) != len(rmsf_data):
                    logging.info(
                        f"Dimension mismatch: unique_resids {len(unique_resids)}, "
                        f"rmsf_data {len(rmsf_data)}"
                    )
                    # Attempt to align by length
                    if len(unique_resids) > len(rmsf_data):
                        logging.warning(
                            f"More unique residues ({len(unique_resids)}) than RMSF points ({len(rmsf_data)}) -- truncating residues"
                        )
                        unique_resids = unique_resids[:len(rmsf_data)]
                        unique_resnames = unique_resnames[:len(rmsf_data)]
                    else:
                        logging.warning(
                            f"Fewer unique residues ({len(unique_resids)}) than RMSF points ({len(rmsf_data)}) -- truncating RMSF"
                        )
                        rmsf_data = rmsf_data[:len(unique_resids)]
                    logging.info("Using unique residue-level alignment for RMSF data")

                # Create DataFrame with final 1:1 alignment
                df = pd.DataFrame({
                    'domain_id': self.domain_id,
                    'resid': unique_resids,
                    'resname': unique_resnames,
                    f'rmsf_{temperature}': rmsf_data
                })
                
                logging.info(f"Created RMSF DataFrame with {len(df)} rows")
                return df
        except Exception as e:
            logging.error(f"Failed to extract RMSF data: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return None

    def extract_pdb(self) -> Optional[str]:
        """
        Extract PDB data from the H5 file.

        Returns:
            PDB string or None if extraction fails
        """
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if 'pdb' not in f[self.domain_id]:
                    logging.error(f"PDB data not found for domain {self.domain_id}")
                    return None
                    
                pdb_data = f[self.domain_id]['pdb'][()]
                if isinstance(pdb_data, bytes):
                    pdb_str = pdb_data.decode('utf-8')
                else:
                    pdb_str = str(pdb_data)
                    
                # Log PDB extraction
                num_lines = pdb_str.count('\n') + 1
                logging.info(f"Extracted PDB data with {num_lines} lines for domain {self.domain_id}")
                
                return pdb_str
        except Exception as e:
            logging.error(f"Failed to extract PDB data: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return None

    def extract_dssp(self, temperature: str, replica: str, frame: int = -1) -> Optional[pd.DataFrame]:
        """
        Extract DSSP data for a specific temperature, replica, and frame.
        DSSP is per-residue, so we build a unique residue-level list
        from the full 'resid'/'resname' arrays. Then align to DSSP codes.
        
        Args:
            temperature: Temperature (e.g., "320")
            replica: Replica (e.g., "0")
            frame: Frame index (default: -1 for last frame)

        Returns:
            DataFrame [domain_id, resid, resname, dssp] or None if extraction fails
        """
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if (temperature not in f[self.domain_id]) or (replica not in f[self.domain_id][temperature]):
                    logging.warning(f"Temperature {temperature} or replica {replica} not found for domain {self.domain_id}")
                    return None

                if 'dssp' not in f[self.domain_id][temperature][replica]:
                    logging.warning(f"DSSP data not found for domain {self.domain_id}, temperature {temperature}, replica {replica}")
                    return None
                    
                dssp_dataset = f[self.domain_id][temperature][replica]['dssp']

                # Number of frames
                num_frames = dssp_dataset.shape[0] if len(dssp_dataset.shape) > 0 else 0
                if num_frames == 0:
                    logging.warning(f"Empty DSSP dataset for domain {self.domain_id}, temperature {temperature}, replica {replica}")
                    return None

                # Convert negative frame index
                if frame < 0:
                    frame = num_frames + frame
                if frame < 0 or frame >= num_frames:
                    logging.warning(f"Frame index {frame} out of bounds (0-{num_frames-1}) for {self.domain_id}")
                    frame = max(0, min(frame, num_frames-1))  # clamp

                dssp_data = dssp_dataset[frame]

                # Full, per-atom arrays
                resids_all = f[self.domain_id]['resid'][:]
                resnames_all = f[self.domain_id]['resname'][:]
                resnames_all = [
                    rn.decode("utf-8") if isinstance(rn, bytes) else str(rn)
                    for rn in resnames_all
                ]

                # Build unique residue-level list
                residue_dict = {}
                for i, resid_val in enumerate(resids_all):
                    if resid_val not in residue_dict:
                        residue_dict[resid_val] = resnames_all[i]

                unique_resids = sorted(residue_dict.keys())
                unique_resnames = [residue_dict[rid] for rid in unique_resids]

                # DSSP codes might already be length = # of residues
                dssp_codes = [
                    c.decode("utf-8") if isinstance(c, bytes) else str(c)
                    for c in dssp_data
                ]

                if len(unique_resids) != len(dssp_codes):
                    logging.info(
                        f"Dimension mismatch in DSSP: unique_resids {len(unique_resids)}, dssp_codes {len(dssp_codes)}"
                    )
                    if len(unique_resids) > len(dssp_codes):
                        logging.warning(
                            f"More unique residues ({len(unique_resids)}) than DSSP codes ({len(dssp_codes)}) -- truncating residues"
                        )
                        unique_resids = unique_resids[:len(dssp_codes)]
                        unique_resnames = unique_resnames[:len(dssp_codes)]
                    else:
                        logging.warning(
                            f"Fewer unique residues ({len(unique_resids)}) than DSSP codes ({len(dssp_codes)}) -- truncating DSSP codes"
                        )
                        dssp_codes = dssp_codes[:len(unique_resids)]
                    logging.info("Using unique residue-level alignment for DSSP data")

                # Create final DataFrame
                df = pd.DataFrame({
                    'domain_id': self.domain_id,
                    'resid': unique_resids,
                    'resname': unique_resnames,
                    'dssp': dssp_codes
                })

                return df

        except Exception as e:
            logging.error(f"Failed to extract DSSP data: {e}")
            return None


    def extract_coordinates(self, temperature: str, replica: str, frame: int = -1) -> Optional[Tuple[np.ndarray, List[int], List[str], np.ndarray, np.ndarray]]:
        """
        Extract coordinate data for a specific temperature, replica, and frame.
        Now includes additional data for frame selection (RMSD and gyration radius)

        Args:
            temperature: Temperature (e.g., "320")
            replica: Replica (e.g., "0")
            frame: Frame index (default: -1 for last frame, -999 for all frames)

        Returns:
            Tuple of (coords, resids, resnames, rmsd_data, gyration_data) where:
            - coords shape is (n_atoms, 3) or (n_frames, n_atoms, 3) if all frames requested
            - rmsd_data shape is (n_frames,)
            - gyration_data shape is (n_frames,)
            or None if extraction fails.
        """
        try:
            with h5py.File(self.h5_path, 'r') as f:
                # Enhanced validation and logging
                logging.info(f"Extracting coordinates for domain: {self.domain_id}")
                logging.info(f"Domain attributes: numChains={f[self.domain_id].attrs.get('numChains', 'N/A')}, "
                            f"numProteinAtoms={f[self.domain_id].attrs.get('numProteinAtoms', 'N/A')}, "
                            f"numResidues={f[self.domain_id].attrs.get('numResidues', 'N/A')}")
                
                # Log atomic numbers (z) information
                if 'z' in f[self.domain_id]:
                    z_data = f[self.domain_id]['z']
                    logging.info(f"z.shape: {z_data.shape}")
                    logging.info(f"First 10 z values: {z_data[:10]}")
                
                # Count CA atoms from PDB data to verify consistency
                if 'pdbProteinAtoms' in f[self.domain_id]:
                    pdb_data = f[self.domain_id]['pdbProteinAtoms'][()].decode('utf-8').split('\n')
                    ca_count = sum(1 for line in pdb_data if line.startswith('ATOM') and ' CA ' in line)
                    logging.info(f"Number of CA atoms in PDB: {ca_count}")
                    if 'numResidues' in f[self.domain_id].attrs:
                        if ca_count != f[self.domain_id].attrs['numResidues']:
                            logging.warning(f"CA count ({ca_count}) does not match numResidues "
                                        f"({f[self.domain_id].attrs['numResidues']})")
                
                # Check temperature and replica existence
                if (temperature not in f[self.domain_id]) or (replica not in f[self.domain_id][temperature]):
                    logging.warning(f"Temperature {temperature} or replica {replica} not found for domain {self.domain_id}")
                    return None

                # Check coordinates, RMSD and gyration radius availability
                req_datasets = ['coords']
                opt_datasets = ['rmsd', 'gyrationRadius']
                for dataset in req_datasets:
                    if dataset not in f[self.domain_id][temperature][replica]:
                        logging.warning(f"{dataset} data not found for domain {self.domain_id}, "
                                    f"temperature {temperature}, replica {replica}")
                        return None

                # Extract coordinates
                coords_dataset = f[self.domain_id][temperature][replica]['coords']
                num_frames = coords_dataset.shape[0] if coords_dataset.ndim > 0 else 0
                
                if num_frames == 0:
                    logging.warning(f"Empty coords dataset for domain {self.domain_id}, "
                                f"temperature {temperature}, replica {replica}")
                    return None

                # Extract RMSD and gyration radius data for frame selection if available
                rmsd_data = None
                gyration_data = None
                
                if 'rmsd' in f[self.domain_id][temperature][replica]:
                    rmsd_data = f[self.domain_id][temperature][replica]['rmsd'][:]
                else:
                    logging.info(f"RMSD data not available for {self.domain_id}, {temperature}, {replica}")
                    rmsd_data = np.zeros(num_frames)
                    
                if 'gyrationRadius' in f[self.domain_id][temperature][replica]:
                    gyration_data = f[self.domain_id][temperature][replica]['gyrationRadius'][:]
                else:
                    logging.info(f"Gyration radius data not available for {self.domain_id}, {temperature}, {replica}")
                    gyration_data = np.zeros(num_frames)
                
                logging.info(f"Available frames: {num_frames}, RMSD shape: {rmsd_data.shape if rmsd_data is not None else 'N/A'}, "
                            f"Gyration shape: {gyration_data.shape if gyration_data is not None else 'N/A'}")

                # Handle negative frame index or return all frames if requested
                if frame == -999:  # Special value to request all frames
                    coords = coords_dataset[:]
                    logging.info(f"Extracting all {num_frames} frames")
                else:
                    # Convert negative frame index
                    if frame < 0:
                        frame = num_frames + frame
                    if frame < 0 or frame >= num_frames:
                        logging.warning(f"Frame index {frame} out of bounds (0-{num_frames-1}) for domain {self.domain_id}")
                        frame = max(0, min(frame, num_frames - 1))
                    logging.info(f"Extracting single frame {frame}")
                    coords = coords_dataset[frame]  # shape (n_atoms, 3)
                    
                    # Validate coordinate shape
                    if coords.ndim != 2 or coords.shape[1] != 3:
                        logging.error(f"Unexpected coordinate shape: {coords.shape} for domain {self.domain_id}")
                        return None

                # Extract residue information
                resids_all = f[self.domain_id]['resid'][:].tolist()
                resnames_all = f[self.domain_id]['resname'][:]
                resnames_all = [
                    rn.decode("utf-8") if isinstance(rn, bytes) else str(rn)
                    for rn in resnames_all
                ]

                # Check shape alignment for single frame
                coord_atoms = coords.shape[0] if coords.ndim == 2 else coords.shape[1]
                if len(resids_all) != coord_atoms:
                    logging.warning(f"Mismatch between residue IDs ({len(resids_all)}) and "
                                f"coords ({coord_atoms})")
                    min_size = min(len(resids_all), coord_atoms)
                    resids_all = resids_all[:min_size]
                    resnames_all = resnames_all[:min_size]
                    if coords.ndim == 3:  # Multiple frames
                        coords = coords[:, :min_size, :]
                    else:  # Single frame
                        coords = coords[:min_size]

                logging.info(f"Successfully extracted coordinates with shape {coords.shape}")
                return coords, resids_all, resnames_all, rmsd_data, gyration_data

        except Exception as e:
            logging.error(f"Failed to extract coordinate data: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return None

def process_domains(domain_ids: List[str], data_dir: str, config: Dict[str, Any],
                    num_cores: int = 1) -> Dict[str, Any]:
    """
    Process multiple domains in parallel.
    """
    from concurrent.futures import ProcessPoolExecutor, as_completed

    max_cores = os.cpu_count() - 2 if os.cpu_count() > 2 else 1
    n_cores = min(num_cores if num_cores > 0 else max_cores, max_cores)

    results = {}
    with ProcessPoolExecutor(max_workers=n_cores) as executor:
        future_to_domain = {}
        for domain_id in domain_ids:
            h5_path = os.path.join(data_dir, f"mdcath_dataset_{domain_id}.h5")
            if not os.path.exists(h5_path):
                logging.warning(f"H5 file not found for domain {domain_id}")
                continue

            future = executor.submit(_process_single_domain, h5_path, config)
            future_to_domain[future] = domain_id

        for future in as_completed(future_to_domain):
            domain_id = future_to_domain[future]
            try:
                result = future.result()
                results[domain_id] = result
            except Exception as e:
                logging.error(f"Error processing domain {domain_id}: {e}")
                results[domain_id] = {"success": False, "error": str(e)}

    return results

def _process_single_domain(h5_path: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a single domain (helper function for parallel processing).
    Updated to remove DSSP extraction.
    """
    loader = H5DataLoader(h5_path, config)
    domain_id = loader.domain_id

    results = {"domain_id": domain_id, "success": False}

    # Extract RMSF data
    temps = [str(t) for t in config.get("temperatures", [320, 348, 379, 413, 450])]
    num_replicas = config.get("num_replicas", 5)

    rmsf_data = {}
    for temp in temps:
        rmsf_data[temp] = {}
        for r in range(num_replicas):
            replica = str(r)
            df_rmsf = loader.extract_rmsf(temp, replica)
            if df_rmsf is not None:
                rmsf_data[temp][replica] = df_rmsf
    results["rmsf_data"] = rmsf_data

    # Extract PDB data
    pdb_str = loader.extract_pdb()
    if pdb_str:
        results["pdb_data"] = pdb_str


    results["success"] = True
    return results
===== FILE: src/mdcath/core/__init__.py =====
"""
Core data loading and processing functions
"""

===== FILE: src/mdcath/processing/core_exterior.py =====
#!/usr/bin/env python3
"""
Processing module for core/exterior classification.
"""

import os
import logging
import subprocess
import tempfile
import pandas as pd
import numpy as np
import Bio
import shutil
from Bio.PDB import PDBParser, DSSP, ShrakeRupley
from typing import Dict, Any, Optional, List, Tuple


def compute_core_exterior(pdb_file: str, config: Dict[str, Any]) -> Optional[pd.DataFrame]:
    """
    Classify residues as 'core' or 'exterior' based on solvent accessibility.

    Args:
        pdb_file: Path to the cleaned PDB file
        config: Configuration dictionary

    Returns:
        DataFrame with columns 'resid' and 'core_exterior' or None if classification fails
    """
    method = config.get("core_exterior", {}).get("method", "msms")

    if method == "msms":
        return compute_core_exterior_msms(pdb_file, config)
    else:
        return compute_core_exterior_biopython(pdb_file, config)

def compute_core_exterior_msms(pdb_file: str, config: Dict[str, Any]) -> Optional[pd.DataFrame]:
    """
    Use MSMS to classify residues as 'core' or 'exterior'.

    Args:
        pdb_file: Path to the cleaned PDB file
        config: Configuration dictionary

    Returns:
        DataFrame with columns 'resid' and 'core_exterior' or None if MSMS fails
    """
    msms_dir = config.get("core_exterior", {}).get("msms_executable_dir", "./msms_executables")
    # Convert to absolute path
    msms_dir = os.path.abspath(msms_dir)
    ses_threshold = config.get("core_exterior", {}).get("ses_threshold", 1.0)
    protein_name = os.path.basename(pdb_file).split('.')[0]

    try:
        # Create temporary directory for MSMS files
        with tempfile.TemporaryDirectory() as tmp_dir:
            # Paths to MSMS executables and output files
            pdb2xyzr_exe = os.path.join(msms_dir, "pdb_to_xyzr")
            msms_exe = os.path.join(msms_dir, "msms.x86_64Linux2.2.6.1")
            xyzr_file = os.path.join(tmp_dir, f"{protein_name}.xyzr")
            area_base = os.path.join(tmp_dir, f"{protein_name}")
            area_file = f"{area_base}.area"

            # Ensure executables have proper permissions
            try:
                os.chmod(pdb2xyzr_exe, 0o755)  # rwxr-xr-x
                os.chmod(msms_exe, 0o755)      # rwxr-xr-x
            except Exception as e:
                logging.warning(f"Failed to set executable permissions: {e}")

            # Check MSMS executables
            if not os.path.exists(pdb2xyzr_exe) or not os.path.exists(msms_exe):
                logging.warning(f"MSMS executables not found in {msms_dir}, falling back to Biopython")
                return compute_core_exterior_biopython(pdb_file, config)

            # Absolute path for input PDB
            abs_pdb_file = os.path.abspath(pdb_file)
            if not os.path.exists(abs_pdb_file):
                logging.warning(f"PDB file not found: {abs_pdb_file}")
                return compute_core_exterior_biopython(pdb_file, config)

            # Run pdb_to_xyzr with bash shell explicitly
            cmd_xyzr = f"bash {pdb2xyzr_exe} {abs_pdb_file} > {xyzr_file}"
            logging.info(f"Running command: {cmd_xyzr}")
            result = subprocess.run(cmd_xyzr, shell=True, check=False,
                                   stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            if result.returncode != 0 or not os.path.exists(xyzr_file) or os.path.getsize(xyzr_file) == 0:
                logging.warning(f"pdb_to_xyzr failed: {result.stderr.decode()}, falling back to Biopython")
                return compute_core_exterior_biopython(pdb_file, config)

            # Run MSMS with bash shell explicitly
            cmd_msms = f"bash {msms_exe} -if {xyzr_file} -af {area_base}"
            logging.info(f"Running command: {cmd_msms}")
            result = subprocess.run(cmd_msms, shell=True, check=False,
                                   stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            if result.returncode != 0 or not os.path.exists(area_file):
                logging.warning(f"MSMS failed: {result.stderr.decode()}, falling back to Biopython")
                return compute_core_exterior_biopython(pdb_file, config)

            # Rest of the function unchanged...
            # Parse atom-level PDB data
            per_atom_df = parse_pdb_atoms(pdb_file)
            if per_atom_df.empty:
                logging.warning(f"Failed to parse atoms from PDB, falling back to Biopython")
                return compute_core_exterior_biopython(pdb_file, config)

            # Parse MSMS area file
            area_df = parse_area_file(area_file)
            if area_df.empty:
                logging.warning(f"Failed to parse area file, falling back to Biopython")
                return compute_core_exterior_biopython(pdb_file, config)

            # Combine atom data with MSMS results
            if len(area_df) != len(per_atom_df):
                logging.warning(f"Atom count mismatch: {len(area_df)} vs {len(per_atom_df)}, falling back to Biopython")
                return compute_core_exterior_biopython(pdb_file, config)

            # Merge data
            per_atom_df = pd.concat([per_atom_df.reset_index(drop=True),
                                    area_df.reset_index(drop=True)], axis=1)

            # Calculate mean SES per residue
            mean_ses_per_res = per_atom_df.groupby("resid")["SES"].mean()

            # Classify residues as core or exterior
            exterior_residues = mean_ses_per_res[mean_ses_per_res > ses_threshold].index
            resids = mean_ses_per_res.index.tolist()
            core_exterior = ["exterior" if r in exterior_residues else "core" for r in resids]

            # Create final dataframe
            result_df = pd.DataFrame({
                "resid": resids,
                "core_exterior": core_exterior
            })

            return result_df
    except Exception as e:
        logging.warning(f"MSMS processing failed: {e}, falling back to Biopython")
        return compute_core_exterior_biopython(pdb_file, config)
    

def compute_core_exterior_biopython(pdb_file: str, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Use Biopython's SASA calculation to classify residues as 'core' or 'exterior'.

    Args:
        pdb_file: Path to the cleaned PDB file
        config: Configuration dictionary

    Returns:
        DataFrame with columns 'resid' and 'core_exterior'
    """
    try:
        from Bio.PDB import PDBParser, Selection
        from Bio.PDB.SASA import ShrakeRupley

        # Set SASA threshold
        sasa_threshold = config.get("core_exterior", {}).get("sasa_threshold", 20.0)

        # Parse PDB - ensure CRYST1 record first
        try:
            # Fix PDB file to ensure proper CRYST1 record
            corrected_lines = []
            with open(pdb_file, 'r') as f:
                lines = f.readlines()
            
            has_cryst1 = False
            for line in lines:
                if line.startswith("CRYST1"):
                    has_cryst1 = True
                    corrected_lines.append("CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n")
                else:
                    corrected_lines.append(line)
            
            if not has_cryst1:
                corrected_lines.insert(0, "CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n")
            
            # Write to temporary file
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdb", mode="w") as tmp:
                tmp_pdb = tmp.name
                tmp.writelines(corrected_lines)
            
            parser = PDBParser(QUIET=True)
            structure = parser.get_structure("protein", tmp_pdb)
            model = structure[0]
        
        except Exception as e:
            logging.warning(f"Failed to fix CRYST1 record: {e}")
            parser = PDBParser(QUIET=True)
            structure = parser.get_structure("protein", pdb_file)
            model = structure[0]
        
        # Try DSSP first for better solvent accessibility calculation
        try:
            # Get the location of the DSSP executable
            dssp_executable = shutil.which("dssp") or shutil.which("mkdssp")
            if dssp_executable:
                logging.info(f"Using DSSP executable: {dssp_executable}")
                
                # Write fixed PDB to temporary file
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdb", mode="w") as tmp:
                    tmp_pdb = tmp.name
                    
                    # Add CRYST1 record if needed
                    with open(pdb_file, 'r') as f:
                        content = f.readlines()
                    
                    has_cryst1 = False
                    for i, line in enumerate(content):
                        if line.startswith("CRYST1"):
                            has_cryst1 = True
                            content[i] = "CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n"
                            break
                    
                    if not has_cryst1:
                        content.insert(0, "CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n")
                    
                    tmp.writelines(content)
                
                # Run DSSP on fixed PDB
                from Bio.PDB import DSSP
                dssp = DSSP(model, tmp_pdb, dssp=dssp_executable)
                
                # Extract results - DSSP gives accessibility values directly
                results = []
                for chain in model:
                    for residue in chain:
                        if residue.id[0] == " ":  # Standard residue
                            resid = residue.id[1]
                            try:
                                # DSSP key is (chain_id, residue_id)
                                key = (chain.id, (' ', residue.id[1], ' '))
                                if key in dssp:
                                    # Get relative solvent accessibility
                                    rel_acc = dssp[key][3]
                                    # A value > 0.2 (20%) is generally considered accessible
                                    core_exterior = "exterior" if rel_acc > 0.2 else "core"
                                    results.append({
                                        "resid": resid, 
                                        "core_exterior": core_exterior,
                                        "relative_accessibility": rel_acc
                                    })
                                else:
                                    # If residue not found in DSSP, use default
                                    results.append({
                                        "resid": resid, 
                                        "core_exterior": "core",
                                        "relative_accessibility": 0.0
                                    })
                            except Exception as e:
                                logging.warning(f"Error processing DSSP for residue {resid}: {e}")
                                results.append({
                                    "resid": resid, 
                                    "core_exterior": "core",
                                    "relative_accessibility": 0.0
                                })
                
                # Clean up temp file
                if os.path.exists(tmp_pdb):
                    os.remove(tmp_pdb)
                
                if results:
                    logging.info("Successfully used DSSP for core/exterior classification")
                    return pd.DataFrame(results)
            
            # If DSSP fails or no results, fall back to ShrakeRupley
            logging.info("DSSP failed or not available, falling back to ShrakeRupley SASA")
        
        except Exception as e:
            logging.warning(f"DSSP calculation failed: {e}, falling back to ShrakeRupley")
        
        # Fall back to ShrakeRupley SASA calculation
        sr = ShrakeRupley()
        sr.compute(model, level="R")  # Compute at residue level

        # Extract results
        results = []
        for chain in model:
            for residue in chain:
                if residue.id[0] == " ":  # Standard residue
                    resid = residue.id[1]
                    sasa = residue.sasa if hasattr(residue, 'sasa') else 0.0
                    # Normalize SASA to get approximation of relative accessibility
                    # Assuming max SASA is around 100 Å²
                    rel_acc = min(1.0, sasa / 100.0)
                    core_exterior = "exterior" if sasa > sasa_threshold else "core"
                    results.append({
                        "resid": resid, 
                        "core_exterior": core_exterior,
                        "relative_accessibility": rel_acc
                    })

        return pd.DataFrame(results)
    except Exception as e:
        logging.error(f"Biopython SASA calculation failed: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return fallback_core_exterior(pdb_file)
    
def fallback_core_exterior(pdb_file: str) -> pd.DataFrame:
    """
    Fallback method to classify residues when other methods fail.
    Classifies outer 1/3 of residues as exterior, inner 2/3 as core.

    Args:
        pdb_file: Path to the cleaned PDB file

    Returns:
        DataFrame with columns 'resid' and 'core_exterior'
    """
    try:
        # Verify file exists and use absolute path
        abs_pdb_file = os.path.abspath(pdb_file)
        if not os.path.exists(abs_pdb_file):
            logging.error(f"PDB file not found: {abs_pdb_file}")
            # Create dummy data when PDB file is missing
            return pd.DataFrame({
                "resid": list(range(1, 21)),  # Create 20 dummy residues
                "core_exterior": ["core"] * 13 + ["exterior"] * 7,  # 2/3 core, 1/3 exterior
                "relative_accessibility": [0.1] * 13 + [0.7] * 7  # Low for core, high for exterior
            })

        # Parse PDB to get residue information
        residue_df = parse_pdb_residues(pdb_file)
        if residue_df.empty:
            # Create empty DataFrame with required columns
            return pd.DataFrame({
                "resid": list(range(1, 21)),
                "core_exterior": ["core"] * 13 + ["exterior"] * 7,
                "relative_accessibility": [0.1] * 13 + [0.7] * 7
            })

        # Sort by residue ID
        residue_df = residue_df.sort_values("resid")

        # Simple classification: outer 1/3 of residues as exterior, inner 2/3 as core
        total_residues = len(residue_df)
        boundary = int(total_residues * 2/3)

        residue_df["core_exterior"] = ["core"] * total_residues
        residue_df.loc[boundary:, "core_exterior"] = "exterior"
        
        # Add relative accessibility values (0-1 scale)
        residue_df["relative_accessibility"] = 0.1  # Default for core
        residue_df.loc[boundary:, "relative_accessibility"] = 0.7  # Higher for exterior

        return residue_df[["resid", "core_exterior", "relative_accessibility"]]
    except Exception as e:
        logging.error(f"Fallback classification failed: {e}")
        return pd.DataFrame({
            "resid": list(range(1, 21)),
            "core_exterior": ["core"] * 13 + ["exterior"] * 7,
            "relative_accessibility": [0.1] * 13 + [0.7] * 7
        })
        
        
def parse_pdb_residues(pdb_file: str) -> pd.DataFrame:
    """
    Parse a PDB file to extract residue-level information.

    Args:
        pdb_file: Path to the PDB file

    Returns:
        DataFrame with residue information
    """
    try:
        from Bio.PDB import PDBParser

        parser = PDBParser(QUIET=True)
        structure = parser.get_structure("protein", pdb_file)

        records = []
        for model in structure:
            for chain in model:
                chain_id = chain.id
                for residue in chain:
                    if residue.id[0] == " ":  # Standard residue
                        records.append({
                            "resid": residue.id[1],
                            "resname": residue.get_resname(),
                            "chain": chain_id
                        })

        return pd.DataFrame(records)
    except Exception as e:
        logging.error(f"Failed to parse PDB residues: {e}")
        return pd.DataFrame()

def parse_pdb_atoms(pdb_file: str) -> pd.DataFrame:
    """
    Parse a PDB file to extract atom-level information.

    Args:
        pdb_file: Path to the PDB file

    Returns:
        DataFrame with atom information
    """
    try:
        from Bio.PDB import PDBParser

        parser = PDBParser(QUIET=True)
        structure = parser.get_structure("protein", pdb_file)

        records = []
        atom_idx = 0
        for model in structure:
            for chain in model:
                for residue in chain:
                    if residue.id[0] == " ":  # Standard residue
                        res_id = residue.id[1]
                        res_name = residue.get_resname()
                        for atom in residue:
                            atom_idx += 1
                            records.append({
                                "atom_idx": atom_idx,
                                "resid": res_id,
                                "resname": res_name,
                                "atom_name": atom.get_name()
                            })

        return pd.DataFrame(records)
    except Exception as e:
        logging.error(f"Failed to parse PDB atoms: {e}")
        return pd.DataFrame()

def parse_area_file(area_file: str) -> pd.DataFrame:
    """
    Parse an MSMS .area file to extract SES values per atom.

    Args:
        area_file: Path to the MSMS .area file

    Returns:
        DataFrame with SES values
    """
    try:
        atom_idx = []
        ses = []

        with open(area_file, "r") as f:
            for line in f:
                if "Atom" in line or not line.strip():
                    continue

                cols = line.split()
                if len(cols) >= 2:
                    atom_idx.append(int(cols[0]))
                    ses.append(float(cols[1]))

        return pd.DataFrame({"atom_idx": atom_idx, "SES": ses})
    except Exception as e:
        logging.error(f"Failed to parse area file: {e}")
        return pd.DataFrame()

def run_dssp_analysis(pdb_file: str) -> pd.DataFrame:
    """
    Run DSSP using a temporary PDB file with correct CRYST1 record,
    then parse the resulting DSSP object.
    
    Args:
        pdb_file: Path to the PDB file
        
    Returns:
        DataFrame with columns: domain_id, resid, chain, dssp, relative_accessibility
    """
    logging.info(f"Running DSSP on {pdb_file}")
    
    # Extract domain_id from filename
    domain_id = os.path.basename(pdb_file).split('.')[0]
    
    # First, verify the PDB file exists
    abs_pdb_file = os.path.abspath(pdb_file)
    if not os.path.exists(abs_pdb_file):
        logging.error(f"PDB file not found: {abs_pdb_file}")
        return use_fallback_dssp(pdb_file)
    
    try:
        # Create a properly formatted CRYST1 record
        corrected_lines = []
        
        # Read original PDB file
        with open(abs_pdb_file, 'r') as f:
            lines = f.readlines()
        
        # Check if CRYST1 record exists and is properly formatted
        has_cryst1 = False
        for i, line in enumerate(lines):
            if line.startswith("CRYST1"):
                has_cryst1 = True
                # Replace with properly formatted CRYST1 record
                corrected_lines.append("CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n")
            else:
                corrected_lines.append(line)
        
        # Add CRYST1 if missing
        if not has_cryst1:
            corrected_lines.insert(0, "CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n")
        
        # Write corrected PDB to temporary file
        tmp_pdb = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdb", mode="w") as tmp_file:
                tmp_pdb = tmp_file.name
                tmp_file.writelines(corrected_lines)
            
            # Run DSSP on corrected file
            from Bio.PDB import PDBParser, DSSP
            parser = PDBParser(QUIET=True)
            structure = parser.get_structure("protein", tmp_pdb)
            model = structure[0]
            
            # Try different DSSP executables
            dssp_found = False
            dssp_obj = None
            
            for dssp_exec in ["dssp", "mkdssp"]:
                dssp_path = shutil.which(dssp_exec)
                if dssp_path:
                    try:
                        logging.info(f"Trying DSSP executable: {dssp_path}")
                        dssp_obj = DSSP(model, tmp_pdb, dssp=dssp_path)
                        dssp_found = True
                        
                        # Log secondary structure distribution
                        ss_counts = {}
                        for key in dssp_obj.keys():
                            ss = dssp_obj[key][2]  # DSSP code
                            if ss not in ss_counts:
                                ss_counts[ss] = 0
                            ss_counts[ss] += 1
                        
                        logging.info(f"DSSP secondary structure counts for {domain_id}: {ss_counts}")
                        break
                    except Exception as e:
                        logging.warning(f"Failed with {dssp_exec}: {e}")
            
            if not dssp_found or dssp_obj is None:
                logging.warning(f"No DSSP executable found or all failed for {domain_id}")
                return use_fallback_dssp(pdb_file)
            
            # Extract DSSP results
            records = []
            for key in dssp_obj.keys():
                chain_id = key[0]
                resid = key[1][1]  # residue number
                dssp_tuple = dssp_obj[key]
                
                # Extract secondary structure and relative accessibility
                ss_code = dssp_tuple[2]  # Secondary structure code
                rel_acc = dssp_tuple[3]  # Relative accessibility
                
                # Ensure secondary structure is never empty
                if not ss_code or ss_code == ' ' or ss_code == '-':
                    ss_code = 'C'  # Default to coil
                
                records.append({
                    "domain_id": domain_id,
                    "resid": resid,
                    "chain": chain_id,
                    "dssp": ss_code,
                    "relative_accessibility": rel_acc
                })
            
            if not records:
                logging.warning(f"DSSP returned no records for {domain_id}")
                return use_fallback_dssp(pdb_file)
                
            dssp_df = pd.DataFrame(records)
            logging.info(f"DSSP successfully extracted data for {len(dssp_df)} residues in {domain_id}")
            logging.info(f"DSSP codes distribution for {domain_id}: {dssp_df['dssp'].value_counts().to_dict()}")
            
            return dssp_df
        
        finally:
            # Clean up temporary file
            if tmp_pdb and os.path.exists(tmp_pdb):
                try:
                    os.remove(tmp_pdb)
                except Exception as e:
                    logging.warning(f"Failed to remove temporary file {tmp_pdb}: {e}")
    
    except Exception as e:
        logging.error(f"Failed to run DSSP analysis for {domain_id}: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return use_fallback_dssp(pdb_file)


def use_fallback_dssp(pdb_file: str) -> pd.DataFrame:
    """
    Fallback method when DSSP fails.
    Provides default secondary structure and accessibility values.
    
    Args:
        pdb_file: Path to the PDB file
        
    Returns:
        DataFrame with columns: domain_id, resid, chain, dssp, relative_accessibility
    """
    # Extract domain_id from filename
    domain_id = os.path.basename(pdb_file).split('.')[0]
    
    logging.info(f"Using fallback secondary structure prediction for {domain_id}")
    
    try:
        # First check if the PDB file exists
        abs_pdb_file = os.path.abspath(pdb_file)
        if not os.path.exists(abs_pdb_file):
            # Create dummy data for missing PDB
            dummy_df = pd.DataFrame({
                "domain_id": [domain_id] * 20,
                "resid": list(range(1, 21)),  # 20 dummy residues
                "chain": ["A"] * 20,
                "dssp": ["C"] * 20,
                "relative_accessibility": [0.5] * 20  # Medium accessibility
            })
            logging.warning(f"PDB file not found for {domain_id}, using dummy data with {len(dummy_df)} residues")
            return dummy_df
        
        # Parse PDB to get residue info
        try:
            from Bio.PDB import PDBParser
            parser = PDBParser(QUIET=True)
            structure = parser.get_structure("protein", abs_pdb_file)
            
            records = []
            for model in structure:
                for chain in model:
                    chain_id = chain.id
                    for residue in chain:
                        if residue.id[0] == " ":  # Standard residue
                            resid = residue.id[1]
                            records.append({
                                "domain_id": domain_id,
                                "resid": resid,
                                "chain": chain_id,
                                "dssp": "C",  # Default to coil
                                "relative_accessibility": 0.5  # Default to moderate accessibility
                            })
            
            if records:
                result_df = pd.DataFrame(records)
                logging.info(f"Created fallback DSSP data for {domain_id} with {len(result_df)} residues")
                return result_df
        except Exception as e:
            logging.warning(f"Failed to parse PDB structure for {domain_id}: {e}")
        
        # If we get here, we couldn't parse the PDB, so create dummy data
        dummy_df = pd.DataFrame({
            "domain_id": [domain_id] * 20,
            "resid": list(range(1, 21)),
            "chain": ["A"] * 20,
            "dssp": ["C"] * 20,
            "relative_accessibility": [0.5] * 20
        })
        logging.warning(f"Failed to parse PDB for {domain_id}, using dummy data with {len(dummy_df)} residues")
        return dummy_df
        
    except Exception as e:
        logging.error(f"Fallback DSSP also failed for {domain_id}: {e}")
        import traceback
        logging.error(traceback.format_exc())
        
        # Return minimal dataframe with required columns
        dummy_df = pd.DataFrame({
            "domain_id": [domain_id] * 20,
            "resid": list(range(1, 21)),
            "chain": ["A"] * 20,
            "dssp": ["C"] * 20,
            "relative_accessibility": [0.5] * 20
        })
        logging.warning(f"Critical failure in DSSP processing for {domain_id}, using emergency dummy data")
        return dummy_df
===== FILE: src/mdcath/processing/voxelizer.py =====
#!/usr/bin/env python3
"""
Processing module for voxelizing protein structures using aposteriori.
"""

import os
import logging
import subprocess
import traceback
import sys
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed

def voxelize_domains(pdb_results: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Voxelize multiple domains by processing the PDB directory directly.
    Handles the interactive confirmation prompt automatically.
    
    Args:
        pdb_results: Dictionary with PDB processing results
        config: Configuration dictionary
    
    Returns:
        Dictionary with voxelization results
    """
    from tqdm import tqdm
    import glob
    import shutil
    
    output_dir = config.get("output", {}).get("base_dir", "./outputs")
    voxel_dir = os.path.join(output_dir, "voxelized")
    pdb_dir = os.path.join(output_dir, "pdbs")
    
    os.makedirs(voxel_dir, exist_ok=True)
    
    # Check if aposteriori's make-frame-dataset is available
    aposteriori_path = shutil.which("make-frame-dataset")
    aposteriori_available = aposteriori_path is not None
    
    if not aposteriori_available:
        logging.error("make-frame-dataset command not found. Please install aposteriori with: pip install aposteriori")
        return {"success": False, "error": "aposteriori not available"}
    
    logging.info(f"Using aposteriori's make-frame-dataset from: {aposteriori_path}")
    
    # Count valid PDB files
    pdb_files = glob.glob(os.path.join(pdb_dir, "*.pdb"))
    
    if not pdb_files:
        logging.error(f"No PDB files found for voxelization in directory: {pdb_dir}")
        return {"success": False, "error": "No PDB files found"}
    
    logging.info(f"Found {len(pdb_files)} PDB files for voxelization in {pdb_dir}")
    
    # Get voxelization parameters from config
    voxel_config = config.get("processing", {}).get("voxelization", {})
    frame_edge_length = voxel_config.get("frame_edge_length", 12.0)
    voxels_per_side = voxel_config.get("voxels_per_side", 21)
    atom_encoder = voxel_config.get("atom_encoder", "CNOCBCA")
    encode_cb = voxel_config.get("encode_cb", True)
    compression_gzip = voxel_config.get("compression_gzip", True)
    voxelise_all_states = voxel_config.get("voxelise_all_states", False)
    
    # Create output name
    output_name = "mdcath_voxelized"
    
    # Build aposteriori command to process the PDB directory
    cmd = [
        aposteriori_path,
        "-o", voxel_dir,  # Output directory
        "-n", output_name,  # Output filename
        "-v",  # Enable verbose output
        "-e", ".pdb",  # File extension to look for
        "--frame-edge-length", str(frame_edge_length),
        "--voxels-per-side", str(voxels_per_side),
        "-ae", atom_encoder,
        "-cb", str(encode_cb).lower(),
        "-comp", str(compression_gzip).lower(),
        "-vas", str(voxelise_all_states).lower(),
        pdb_dir  # Directory containing PDB files
    ]
    
    # Log the exact command being run
    logging.info(f"Running aposteriori command: {' '.join(cmd)}")
    
    try:
        # Use Popen instead of run to handle interactive prompts
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Send 'y' to confirm when prompted, with a newline character
        stdout, stderr = process.communicate(input='y\n')
        
        # Log the output
        logging.info(f"aposteriori stdout:\n{stdout}")
        if stderr:
            logging.info(f"aposteriori stderr:\n{stderr}")
        
        # Check return code
        if process.returncode != 0:
            logging.error(f"aposteriori command failed with return code {process.returncode}")
        
        # Check for expected output files (try both .hdf5 and .h5 extensions)
        output_file_hdf5 = os.path.join(voxel_dir, f"{output_name}.hdf5")
        output_file_h5 = os.path.join(voxel_dir, f"{output_name}.h5")
        
        if os.path.exists(output_file_hdf5):
            logging.info(f"Voxelization complete. Output: {output_file_hdf5}")
            file_size_mb = os.path.getsize(output_file_hdf5) / (1024 * 1024)
            logging.info(f"Output file size: {file_size_mb:.2f} MB")
            
            # Count number of residue frames
            try:
                import h5py
                with h5py.File(output_file_hdf5, 'r') as f:
                    num_frames = 0
                    for pdb_code in f.keys():
                        for chain_id in f[pdb_code].keys():
                            num_frames += len(f[pdb_code][chain_id].keys())
                logging.info(f"Dataset contains {num_frames} residue frames")
            except Exception as e:
                logging.warning(f"Could not count frames in output file: {e}")
            
            return {
                "success": True,
                "output_file": output_file_hdf5,
                "file_size_mb": file_size_mb,
                "num_frames": num_frames if 'num_frames' in locals() else None
            }
        elif os.path.exists(output_file_h5):
            logging.info(f"Voxelization complete. Output: {output_file_h5}")
            file_size_mb = os.path.getsize(output_file_h5) / (1024 * 1024)
            logging.info(f"Output file size: {file_size_mb:.2f} MB")
            
            # Count number of residue frames
            try:
                import h5py
                with h5py.File(output_file_h5, 'r') as f:
                    num_frames = 0
                    for pdb_code in f.keys():
                        for chain_id in f[pdb_code].keys():
                            num_frames += len(f[pdb_code][chain_id].keys())
                logging.info(f"Dataset contains {num_frames} residue frames")
            except Exception as e:
                logging.warning(f"Could not count frames in output file: {e}")
            
            return {
                "success": True,
                "output_file": output_file_h5,
                "file_size_mb": file_size_mb,
                "num_frames": num_frames if 'num_frames' in locals() else None
            }
        else:
            logging.error(f"Voxelization failed: No output file generated")
            return {
                "success": False,
                "error": "Output file not found",
                "command": ' '.join(cmd),
                "stdout": stdout,
                "stderr": stderr
            }
        
    except Exception as e:
        logging.error(f"Voxelization error: {e}")
        logging.error(traceback.format_exc())
        return {
            "success": False, 
            "error": str(e)
        }
===== FILE: src/mdcath/processing/__init__.py =====
"""
Data processing modules for mdCATH
"""

===== FILE: src/mdcath/processing/rmsf.py =====
#!/usr/bin/env python3
"""
Processing module for RMSF data extraction and averaging.
"""

import os
import logging
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Any, Union
from concurrent.futures import ProcessPoolExecutor

def calculate_replica_averages(rmsf_data: Dict[str, Dict[str, pd.DataFrame]],
                              temperature: str) -> Optional[pd.DataFrame]:
    """
    Calculate average RMSF across all replicas for a specific temperature.

    Args:
        rmsf_data: Dictionary with RMSF data for all replicas
        temperature: Temperature to calculate average for

    Returns:
        DataFrame with average RMSF values or None if calculation fails
    """
    try:
        # Collect all dataframes for this temperature
        dfs = []
        for replica, df in rmsf_data.get(temperature, {}).items():
            if df is not None:
                dfs.append(df)

        if not dfs:
            logging.warning(f"No RMSF data found for temperature {temperature}")
            return None

        # Combine the first dataframe for residue information
        result_df = dfs[0][['domain_id', 'resid', 'resname']].copy()

        # Calculate average RMSF
        rmsf_values = []
        for df in dfs:
            rmsf_col = f"rmsf_{temperature}"
            if rmsf_col in df.columns:
                rmsf_values.append(df[rmsf_col].values)

        if not rmsf_values:
            logging.warning(f"No RMSF values found for temperature {temperature}")
            return None

        # Calculate average
        avg_rmsf = np.mean(rmsf_values, axis=0)
        result_df[f"rmsf_{temperature}"] = avg_rmsf

        return result_df
    except Exception as e:
        logging.error(f"Failed to calculate replica averages for temperature {temperature}: {e}")
        return None

def calculate_temperature_average(replica_averages: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:
    """
    Calculate average RMSF across all temperatures.

    Args:
        replica_averages: Dictionary with replica average RMSF data for all temperatures

    Returns:
        DataFrame with average RMSF values across all temperatures or None if calculation fails
    """
    try:
        if not replica_averages:
            logging.warning("No replica averages found")
            return None

        # Get the first dataframe for base structure
        temps = list(replica_averages.keys())
        first_temp = temps[0]
        result_df = replica_averages[first_temp][['domain_id', 'resid', 'resname']].copy()

        # Collect RMSF values for all temperatures
        rmsf_cols = []
        for temp, df in replica_averages.items():
            rmsf_col = f"rmsf_{temp}"
            if rmsf_col in df.columns:
                result_df[rmsf_col] = df[rmsf_col]
                rmsf_cols.append(rmsf_col)

        if not rmsf_cols:
            logging.warning("No RMSF columns found")
            return None

        # Calculate average across all temperatures
        result_df['rmsf_average'] = result_df[rmsf_cols].mean(axis=1)

        return result_df
    except Exception as e:
        logging.error(f"Failed to calculate temperature average: {e}")
        return None

def save_rmsf_data(rmsf_data: Dict[str, Dict[str, pd.DataFrame]],
                  replica_averages: Dict[str, pd.DataFrame],
                  temperature_average: pd.DataFrame,
                  output_dir: str) -> bool:
    """
    Save RMSF data to CSV files.

    Args:
        rmsf_data: Dictionary with RMSF data for all temperatures and replicas
        replica_averages: Dictionary with replica average RMSF data for all temperatures
        temperature_average: DataFrame with average RMSF values across all temperatures
        output_dir: Directory to save CSV files

    Returns:
        Boolean indicating if saving was successful
    """
    try:
        # Create output directory structure
        os.makedirs(os.path.join(output_dir, "RMSF", "replicas"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "RMSF", "replica_average", "average"), exist_ok=True)

        # Save replica data
        for temp, replicas in rmsf_data.items():
            for replica, df in replicas.items():
                replica_dir = os.path.join(output_dir, "RMSF", "replicas", f"replica_{replica}", temp)
                os.makedirs(replica_dir, exist_ok=True)

                output_file = os.path.join(replica_dir, f"rmsf_replica{replica}_temperature{temp}.csv")
                df.to_csv(output_file, index=False)
                logging.info(f"Saved RMSF data to {output_file}")

        # Save replica averages
        for temp, df in replica_averages.items():
            temp_dir = os.path.join(output_dir, "RMSF", "replica_average", temp)
            os.makedirs(temp_dir, exist_ok=True)

            output_file = os.path.join(temp_dir, f"rmsf_replica_average_temperature{temp}.csv")
            df.to_csv(output_file, index=False)
            logging.info(f"Saved replica average RMSF data to {output_file}")

        # Save temperature average
        output_file = os.path.join(output_dir, "RMSF", "replica_average", "average",
                                  "rmsf_all_temperatures_all_replicas.csv")
        temperature_average.to_csv(output_file, index=False)
        logging.info(f"Saved temperature average RMSF data to {output_file}")

        return True
    except Exception as e:
        logging.error(f"Failed to save RMSF data: {e}")
        return False

def process_rmsf_data(domain_results: Dict[str, Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process RMSF data for all domains.

    Args:
        domain_results: Dictionary with processing results for all domains
        config: Configuration dictionary

    Returns:
        Dictionary with RMSF processing results
    """
    temps = [str(t) for t in config.get("temperatures", [320, 348, 379, 413, 450])]
    output_dir = config.get("output", {}).get("base_dir", "./outputs")

    # Combine RMSF data from all domains
    combined_rmsf_data = {temp: {} for temp in temps}
    for domain_id, result in domain_results.items():
        if not result.get("success", False):
            continue

        rmsf_data = result.get("rmsf_data", {})
        for temp in temps:
            if temp in rmsf_data:
                for replica, df in rmsf_data[temp].items():
                    if replica not in combined_rmsf_data[temp]:
                        combined_rmsf_data[temp][replica] = []
                    combined_rmsf_data[temp][replica].append(df)

    # Concatenate dataframes for each temperature and replica
    for temp in combined_rmsf_data:
        for replica in combined_rmsf_data[temp]:
            if combined_rmsf_data[temp][replica]:
                combined_rmsf_data[temp][replica] = pd.concat(combined_rmsf_data[temp][replica], ignore_index=True)

    # Calculate replica averages
    replica_averages = {}
    for temp in temps:
        avg_df = calculate_replica_averages(combined_rmsf_data, temp)
        if avg_df is not None:
            replica_averages[temp] = avg_df

    # Calculate temperature average
    temperature_average = calculate_temperature_average(replica_averages)

    # Save RMSF data
    save_success = save_rmsf_data(combined_rmsf_data, replica_averages, temperature_average, output_dir)

    return {
        "combined_rmsf_data": combined_rmsf_data,
        "replica_averages": replica_averages,
        "temperature_average": temperature_average,
        "save_success": save_success
    }

===== FILE: src/mdcath/processing/pdb.py =====
#!/usr/bin/env python3
"""
Processing module for PDB data extraction and cleaning.
"""

import os
import logging
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed

# Import pdbUtils for better PDB handling
try:
    from pdbUtils import pdbUtils
    PDBUTILS_AVAILABLE = True
except ImportError:
    logging.warning("pdbUtils library not found. Installing fallback method for PDB cleaning.")
    PDBUTILS_AVAILABLE = False


def save_pdb_file(pdb_string: str, output_path: str, config: Dict[str, Any]) -> bool:
    """
    Save a PDB string to a file with cleaning applied.

    Args:
        pdb_string: PDB data as a string
        output_path: Path to save the cleaned PDB
        config: Configuration dictionary

    Returns:
        Boolean indicating if saving was successful
    """
    try:
        # Write original PDB to a temporary file
        temp_path = output_path + ".temp"
        with open(temp_path, 'w') as f:
            f.write(pdb_string)

        # Clean the PDB file
        success = fix_pdb(temp_path, output_path, config)

        # Remove temporary file
        if os.path.exists(temp_path):
            os.remove(temp_path)

        return success
    except Exception as e:
        logging.error(f"Failed to save PDB file: {e}")
        return False


def fix_pdb(input_pdb_path: str, output_pdb_path: str, config: Dict[str, Any]) -> bool:
    """
    Clean and fix a PDB file for downstream processing using pdbUtils or fallback.

    Args:
        input_pdb_path: Path to input PDB file
        output_pdb_path: Path to save the cleaned PDB file
        config: Configuration dictionary

    Returns:
        Boolean indicating if cleaning was successful
    """
    if not os.path.isfile(input_pdb_path):
        logging.error(f"PDB file not found: {input_pdb_path}")
        return False

    try:
        if PDBUTILS_AVAILABLE:
            return fix_pdb_with_pdbutils(input_pdb_path, output_pdb_path, config)
        else:
            return fix_pdb_fallback(input_pdb_path, output_pdb_path, config)
    except Exception as e:
        logging.error(f"Failed to clean PDB {input_pdb_path}: {e}")
        return False


def fix_pdb_with_pdbutils(input_pdb_path: str, output_pdb_path: str, config: Dict[str, Any]) -> bool:
    """
    Clean PDB file using the pdbUtils library for better compatibility with your pipeline.

    Args:
        input_pdb_path: Path to input PDB file
        output_pdb_path: Path to save the cleaned PDB file
        config: Configuration dictionary

    Returns:
        Boolean indicating if cleaning was successful
    """
    try:
        # First, check if we need to stop after TER
        stop_after_ter = config.get("pdb_cleaning", {}).get("stop_after_ter", True)
        
        # If we need to stop after TER, we need to process the file line by line first
        if stop_after_ter:
            with open(input_pdb_path, 'r') as f:
                lines = f.readlines()
                
            # Find the first TER marker
            ter_index = -1
            for i, line in enumerate(lines):
                if line.startswith("TER"):
                    ter_index = i
                    break
            
            # Create a temporary file with content up to TER marker (or all if no TER found)
            temp_input_path = input_pdb_path + ".preproc"
            with open(temp_input_path, 'w') as f:
                if ter_index > 0:
                    f.writelines(lines[:ter_index+1])  # Include the TER line
                    if not any(line.startswith("END") for line in lines[:ter_index+1]):
                        f.write("END\n")  # Add END if not present
                else:
                    f.writelines(lines)  # Use all lines if no TER found
                    
            # Now use this as our input
            actual_input = temp_input_path
        else:
            actual_input = input_pdb_path
            
        # Convert PDB to DataFrame using pdbUtils
        pdb_df = pdbUtils.pdb2df(actual_input)
        initial_atoms = len(pdb_df)

        clean_config = config.get("pdb_cleaning", {})

        # # Fix ONLY the first CAY to N - more targeted approach
        # if clean_config.get("fix_first_cay", True):
        #     atom_name_col = "ATOM_NAME" if "ATOM_NAME" in pdb_df.columns else "atom_name"
        #     if atom_name_col in pdb_df.columns:
        #         # Find the first CAY atom
        #         cay_rows = pdb_df[pdb_df[atom_name_col] == "CAY"]
        #         if not cay_rows.empty:
        #             first_cay_idx = cay_rows.index[0]
        #             # Change only the first CAY to N
        #             pdb_df.at[first_cay_idx, atom_name_col] = "N"
        #             logging.info("Changed first CAY atom to N")

        # Replace HSD/HSE/HSP with HIS
        if clean_config.get("correct_unusual_residue_names", True):
            if "RES_NAME" in pdb_df.columns:
                pdb_df["RES_NAME"] = pdb_df["RES_NAME"].apply(
                    lambda x: "HIS" if str(x).strip() in ["HSD", "HSE", "HSP"] else x
                )
            elif "resName" in pdb_df.columns:
                pdb_df["resName"] = pdb_df["resName"].apply(
                    lambda x: "HIS" if str(x).strip() in ["HSD", "HSE", "HSP"] else x
                )

        # Replace chain 0 with A
        if clean_config.get("replace_chain_0_with_A", True):
            chain_col = "CHAIN_ID" if "CHAIN_ID" in pdb_df.columns else "chainID"
            if chain_col in pdb_df.columns:
                pdb_df[chain_col] = pdb_df[chain_col].apply(
                    lambda x: "A" if str(x).strip() == "0" else x
                )

        # Fix atom numbering
        if clean_config.get("fix_atom_numbering", True):
            atom_num_col = "ATOM_NUM" if "ATOM_NUM" in pdb_df.columns else "atomNum"
            if atom_num_col in pdb_df.columns:
                pdb_df[atom_num_col] = range(1, len(pdb_df) + 1)

        # Remove hydrogens if specified
        if clean_config.get("remove_hydrogens", False):
            elem_col = "ELEMENT" if "ELEMENT" in pdb_df.columns else "element"
            if elem_col in pdb_df.columns:
                pdb_df = pdb_df[pdb_df[elem_col] != "H"]

        # Remove water/ions if config says so (TIP, WAT, HOH, SOD, CLA, chain W)
        if clean_config.get("remove_solvent_ions", False):
            skip_resnames = {"TIP", "WAT", "HOH", "SOD", "CLA"}
            res_col = "RES_NAME" if "RES_NAME" in pdb_df.columns else "resName"
            chain_col = "CHAIN_ID" if "CHAIN_ID" in pdb_df.columns else "chainID"
            if res_col in pdb_df.columns:
                pdb_df = pdb_df[~pdb_df[res_col].isin(skip_resnames)]
            if chain_col in pdb_df.columns:
                pdb_df = pdb_df[pdb_df[chain_col] != "W"]

        # Save cleaned PDB
        pdbUtils.df2pdb(pdb_df, output_pdb_path)

        # Add properly formatted CRYST1 record for DSSP compatibility
        # This is critical as the error logs show DSSP is failing due to malformed CRYST1
        with open(output_pdb_path, "r") as f:
            content = f.readlines()
        
        # Properly formatted CRYST1 record with proper spacing and floating point values
        cryst1_line = "CRYST1  100.000  100.000  100.000  90.00  90.00  90.00 P 1           1\n"
        
        # Check if CRYST1 already exists and modify/add as needed
        has_cryst1 = False
        for i, line in enumerate(content):
            if line.startswith("CRYST1"):
                has_cryst1 = True
                content[i] = cryst1_line
                break
        
        if not has_cryst1:
            content.insert(0, cryst1_line)
        
        with open(output_pdb_path, "w") as f:
            f.writelines(content)

        # Cleanup temporary file if created
        if stop_after_ter and os.path.exists(temp_input_path):
            os.remove(temp_input_path)

        final_atoms = len(pdb_df)
        logging.info(
            f"Cleaned PDB {os.path.basename(input_pdb_path)}: {initial_atoms} atoms → {final_atoms} atoms"
        )
        return True

    except Exception as e:
        logging.error(f"Failed to clean PDB with pdbUtils: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

def fix_pdb_fallback(input_pdb_path: str, output_pdb_path: str, config: Dict[str, Any]) -> bool:
    """
    Fallback method to clean a PDB file when pdbUtils is not available.

    Args:
        input_pdb_path: Path to input PDB file
        output_pdb_path: Path to save the cleaned PDB file
        config: Configuration dictionary

    Returns:
        Boolean indicating if cleaning was successful
    """
    try:
        with open(input_pdb_path, 'r') as f:
            lines = f.readlines()

        clean_config = config.get("pdb_cleaning", {})
        has_cryst1 = any(line.strip().startswith("CRYST1") for line in lines)

        cleaned_lines = []
        # Add CRYST1 if missing
        if not has_cryst1 and clean_config.get("add_cryst1_record", True):
            # Properly formatted CRYST1 record with correct spacing
            cleaned_lines.append("CRYST1    100.000   100.000   100.000  90.00  90.00  90.00 P 1           1\n")

        # We can skip TIP, WAT, HOH, SOD, CLA, chain 'W' if remove_solvent_ions is True
        skip_solvent = clean_config.get("remove_solvent_ions", False)
        skip_resnames = {"TIP", "WAT", "HOH", "SOD", "CLA"}
        stop_after_ter = clean_config.get("stop_after_ter", True)
        
        # Track whether we've encountered TER
        ter_encountered = False
        
        # Fix atom numbering
        atom_num = 1

        for line in lines:
            # Check if we should stop processing (after TER marker)
            if stop_after_ter and ter_encountered:
                # Only keep certain record types after TER (like END)
                if line.startswith("END"):
                    cleaned_lines.append(line)
                continue
                
            # Process TER line if encountered
            if line.startswith("TER"):
                cleaned_lines.append(f"TER   {atom_num:5d}\n")
                if stop_after_ter:
                    ter_encountered = True
                continue
                
            if line.startswith("ATOM") or line.startswith("HETATM"):
                try:
                    record_type = line[0:6].strip()
                    # atom_num from the original is ignored, we use our counter
                    atom_name = line[12:16].strip()
                    alt_loc = line[16:17].strip()
                    res_name = line[17:20].strip()
                    chain_id = line[21:22].strip()
                    res_num = line[22:26].strip()
                    ins_code = line[26:27].strip()
                    x = float(line[30:38].strip())
                    y = float(line[38:46].strip())
                    z = float(line[46:54].strip())
                    occ = line[54:60].strip() or "0.00"
                    temp_factor = line[60:66].strip() or "0.00"
                    # Derive element from atom name if not present
                    element = line[76:78].strip() if len(line) >= 78 else atom_name[0:1]
                    
                    # Fix chain '0' → 'A'
                    if clean_config.get("replace_chain_0_with_A", True) and chain_id == "0":
                        chain_id = "A"

                    # Convert HSD/HSE/HSP → HIS
                    if clean_config.get("correct_unusual_residue_names", True):
                        if res_name in ["HSD", "HSE", "HSP"]:
                            res_name = "HIS"

                    # Remove hydrogens if desired
                    if clean_config.get("remove_hydrogens", False) and (
                        element == "H" or atom_name.startswith("H")
                    ):
                        continue

                    # Remove water/ions if configured
                    if skip_solvent:
                        # If residue name is in skip set OR chain is W, skip
                        if res_name in skip_resnames or chain_id == "W":
                            continue

                    # Format line
                    new_line = (
                        f"{record_type:<6s}{atom_num:5d} {atom_name:<4s}{alt_loc:1s}"
                        f"{res_name:3s} {chain_id:1s}{res_num:4s}{ins_code:1s}"
                        f"   {x:8.3f}{y:8.3f}{z:8.3f}"
                        f"{float(occ):6.2f}{float(temp_factor):6.2f}"
                        f"           {element:>2s}  \n"
                    )
                    cleaned_lines.append(new_line)
                    atom_num += 1
                    
                except ValueError as ve:
                    logging.warning(f"Error parsing line: {line.strip()} -> {ve}")
                    # Skip problematic lines
                    continue
                except Exception as e:
                    logging.warning(f"Other error processing line: {line.strip()} -> {e}")
                    # Skip problematic lines
                    continue
            elif not line.startswith("ATOM") and not line.startswith("HETATM"):
                # Keep non-ATOM lines as is (e.g. REMARK, unless it's after TER)
                cleaned_lines.append(line)

        # Add END record if not present
        if not any(line.startswith("END") for line in cleaned_lines):
            cleaned_lines.append("END\n")
            
        # Write cleaned PDB
        with open(output_pdb_path, 'w') as f:
            f.writelines(cleaned_lines)

        logging.info(f"Cleaned PDB {os.path.basename(input_pdb_path)} → {os.path.basename(output_pdb_path)}, {atom_num-1} atoms")
        return True

    except Exception as e:
        logging.error(f"Failed to clean PDB {input_pdb_path} with fallback method: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False
    
    
def extract_frames(coords: np.ndarray,
                   resids: List[int],
                   resnames: List[str],
                   domain_id: str,
                   output_dir: str,
                   temperature: str,
                   replica: str,
                   config: Dict[str, Any],
                   rmsd_data: Optional[np.ndarray] = None,
                   gyration_data: Optional[np.ndarray] = None) -> bool:
    """
    Extract frames from coordinate data and save as PDB files.
    Uses the cleaned PDB as a template and updates only the coordinates.
    Supports multiple frame selection methods: regular, random, gyration, and rmsd.

    Args:
        coords: Coordinate data (can be single frame or multiple frames)
        resids: Residue IDs
        resnames: Residue names
        domain_id: Domain identifier
        output_dir: Directory to save frame PDBs
        temperature: Temperature
        replica: Replica index
        config: Configuration dictionary
        rmsd_data: RMSD data for frame selection (if available)
        gyration_data: Gyration radius data for frame selection (if available)

    Returns:
        Boolean indicating if extraction was successful
    """
    import numpy as np
    import os
    from sklearn.cluster import KMeans
    import random
    
    frame_selection = config.get("processing", {}).get("frame_selection", {})
    method = frame_selection.get("method", "rmsd")
    num_frames = frame_selection.get("num_frames", 1)
    cluster_method = frame_selection.get("cluster_method", "kmeans")

    try:
        # Create output directory for frames
        frame_dir = os.path.join(output_dir, "frames", f"replica_{replica}", temperature)
        os.makedirs(frame_dir, exist_ok=True)
        
        # Get the source cleaned PDB path to use as a template
        pdb_dir = os.path.join(output_dir, "pdbs")
        pdb_path = os.path.join(pdb_dir, f"{domain_id}.pdb")
        
        # Check if the cleaned PDB exists
        if not os.path.exists(pdb_path):
            logging.error(f"Cleaned PDB file not found for domain {domain_id}: {pdb_path}")
            return False
            
        # Read the cleaned PDB template
        with open(pdb_path, 'r') as f:
            pdb_lines = f.readlines()
        
        # Determine frame indices to extract
        frame_indices = []
        
        # If coordinates are already a single frame (num_frames == 1 case)
        if coords.ndim == 2:
            logging.info(f"Processing single frame for domain {domain_id}")
            frame_indices = [0]  # We already have the frame we want
            # Reshape to make the processing logic consistent
            coords = np.expand_dims(coords, axis=0)
        else:
            # Handle multi-frame selection (num_frames > 1)
            total_frames = coords.shape[0]
            logging.info(f"Selecting {num_frames} frames from {total_frames} available frames using {method} method")
            
            if num_frames >= total_frames:
                # Use all frames if requested number exceeds available
                frame_indices = list(range(total_frames))
                logging.warning(f"Requested {num_frames} frames but only {total_frames} are available")
            else:
                if method == "regular":
                    # Evenly spaced frames
                    step = total_frames // num_frames
                    frame_indices = [i * step for i in range(num_frames)]
                
                elif method == "random":
                    # Random selection without replacement
                    frame_indices = random.sample(range(total_frames), num_frames)
                
                elif method == "gyration":
                    # Select frames based on gyration radius
                    if gyration_data is None or len(gyration_data) != total_frames:
                        logging.warning("Gyration data not available or invalid, falling back to regular sampling")
                        step = total_frames // num_frames
                        frame_indices = [i * step for i in range(num_frames)]
                    else:
                        # Select diverse frames by gyration radius
                        sorted_indices = np.argsort(gyration_data)
                        step = len(sorted_indices) // num_frames
                        frame_indices = [sorted_indices[i * step] for i in range(num_frames)]
                
                elif method == "rmsd":
                    # Selection based on RMSD clustering
                    if rmsd_data is None or len(rmsd_data) != total_frames:
                        logging.warning("RMSD data not available or invalid, falling back to regular sampling")
                        step = total_frames // num_frames
                        frame_indices = [i * step for i in range(num_frames)]
                    else:
                        # Use k-means clustering on RMSD values
                        if cluster_method == "kmeans":
                            # Reshape for k-means
                            rmsd_reshaped = rmsd_data.reshape(-1, 1)
                            
                            # Apply k-means clustering
                            kmeans = KMeans(n_clusters=num_frames, random_state=42, n_init=10)
                            cluster_labels = kmeans.fit_predict(rmsd_reshaped)
                            
                            # Select the frame closest to each cluster center
                            frame_indices = []
                            for i in range(num_frames):
                                cluster_points = np.where(cluster_labels == i)[0]
                                if len(cluster_points) > 0:
                                    # Get the point closest to the cluster center
                                    cluster_center = kmeans.cluster_centers_[i]
                                    distances = np.abs(rmsd_reshaped[cluster_points] - cluster_center)
                                    closest_idx = cluster_points[np.argmin(distances)]
                                    frame_indices.append(closest_idx)
                        else:
                            logging.warning(f"Unsupported cluster method: {cluster_method}, using regular sampling")
                            step = total_frames // num_frames
                            frame_indices = [i * step for i in range(num_frames)]
                else:
                    logging.warning(f"Unsupported frame selection method: {method}, using regular sampling")
                    step = total_frames // num_frames
                    frame_indices = [i * step for i in range(num_frames)]
        
        logging.info(f"Selected frame indices: {frame_indices}")
        
        # Process each selected frame
        success_count = 0
        
        for frame_idx in frame_indices:
            frame_coords = coords[frame_idx]
            
            # Create a mapping from resid to coordinates index
            resid_to_coord = {}
            for i, resid in enumerate(resids):
                if i < len(frame_coords):
                    if resid not in resid_to_coord:
                        resid_to_coord[resid] = []
                    resid_to_coord[resid].append(i)
            
            # Track which atom indices in the PDB have been mapped to coordinates
            atom_coord_mapping = {}
            
            # First pass: Process lines and build atom_resid mapping
            atom_resid_mapping = {}
            for i, line in enumerate(pdb_lines):
                if line.startswith("ATOM") or line.startswith("HETATM"):
                    try:
                        atom_num = int(line[6:11].strip())
                        res_num = int(line[22:26].strip())
                        atom_resid_mapping[atom_num] = res_num
                    except ValueError:
                        continue
            
            # Second pass: Create new PDB with updated coordinates where possible
            new_pdb_lines = []
            for line in pdb_lines:
                if line.startswith("ATOM") or line.startswith("HETATM"):
                    try:
                        # Extract parts of the ATOM line we want to keep
                        record_type = line[0:6].strip()
                        atom_num = int(line[6:11].strip())
                        atom_name = line[12:16].strip()
                        alt_loc = line[16:17].strip()
                        res_name = line[17:20].strip()
                        chain_id = line[21:22].strip()
                        res_num = int(line[22:26].strip())
                        ins_code = line[26:27].strip()
                        
                        # Extract values from original line for fields we're not updating
                        occ = line[54:60].strip() or "0.00"
                        temp_factor = line[60:66].strip() or "0.00"
                        element = line[76:78].strip() if len(line) >= 78 else ""
                        
                        # Look up coordinates based on resid
                        if res_num in resid_to_coord and len(resid_to_coord[res_num]) > 0:
                            coord_idx = resid_to_coord[res_num][0]
                            # Update coordinates
                            x, y, z = frame_coords[coord_idx]
                            # Format new ATOM line with updated coordinates
                            new_line = (
                                f"{record_type:<6s}{atom_num:5d} {atom_name:<4s}{alt_loc:1s}"
                                f"{res_name:3s} {chain_id:1s}{res_num:4d}{ins_code:1s}"
                                f"   {x:8.3f}{y:8.3f}{z:8.3f}"
                                f"{float(occ):6.2f}{float(temp_factor):6.2f}"
                                f"           {element:>2s}  \n"
                            )
                            atom_coord_mapping[atom_num] = coord_idx
                        else:
                            # Keep original coordinates if resid not in coordinate data
                            new_line = line
                        
                        new_pdb_lines.append(new_line)
                    except ValueError:
                        # If we can't parse the line, keep it as is
                        new_pdb_lines.append(line)
                else:
                    # Non-ATOM lines (HEADER, REMARK, etc.) stay the same
                    new_pdb_lines.append(line)
            
            # Check if we've used all coordinates
            coords_used = len(atom_coord_mapping)
            logging.info(f"Used {coords_used}/{len(frame_coords)} coordinates for frame {frame_idx}")
            
            # Write out the frame PDB
            if num_frames == 1:
                frame_path = os.path.join(frame_dir, f"{domain_id}_frame.pdb")
            else:
                frame_path = os.path.join(frame_dir, f"{domain_id}_frame_{frame_idx}.pdb")
                
            with open(frame_path, 'w') as f:
                f.writelines(new_pdb_lines)
            
            success_count += 1
            logging.info(f"Extracted frame {frame_idx} for domain {domain_id} at T={temperature}, rep={replica}")
        
        return success_count > 0

    except Exception as e:
        logging.error(f"Failed to extract frames for domain {domain_id}: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

def process_pdb_data(domain_results: Dict[str, Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process PDB data for all domains: save cleaned PDB, optionally extract frames, etc.
    Updated to handle the enhanced frame selection.

    Args:
        domain_results: Dictionary with processing results for all domains
        config: Configuration dictionary

    Returns:
        Dictionary with PDB processing results
    """
    from src.mdcath.core.data_loader import H5DataLoader
    from tqdm import tqdm

    output_dir = config.get("output", {}).get("base_dir", "./outputs")
    input_dir = config.get("input", {}).get("mdcath_folder", "/mnt/datasets/MD_CATH/data")

    # Create output directories
    pdb_dir = os.path.join(output_dir, "pdbs")
    frames_dir = os.path.join(output_dir, "frames")
    os.makedirs(pdb_dir, exist_ok=True)
    os.makedirs(frames_dir, exist_ok=True)

    results = {}

    # Check for pdbUtils
    if PDBUTILS_AVAILABLE:
        logging.info("Using pdbUtils for PDB cleaning (recommended).")
    else:
        logging.warning("pdbUtils not available, using fallback cleaning method.")

    # Save cleaned PDBs
    logging.info("Processing PDB data for domains...")
    for domain_id, result in tqdm(domain_results.items(), desc="Processing PDB files"):
        if not result.get("success", False):
            continue

        pdb_data = result.get("pdb_data", "")
        if pdb_data:
            pdb_path = os.path.join(pdb_dir, f"{domain_id}.pdb")
            success = save_pdb_file(pdb_data, pdb_path, config)
            if success:
                logging.info(f"Saved cleaned PDB for domain {domain_id}")
                results[domain_id] = {"pdb_saved": True, "pdb_path": pdb_path}
            else:
                logging.error(f"Failed to save cleaned PDB for domain {domain_id}")
                results[domain_id] = {"pdb_saved": False}

    # Extract frames from HDF5 trajectories
    temps = [str(t) for t in config.get("temperatures", [320, 348, 379, 413, 450])]
    num_replicas = config.get("num_replicas", 5)
    num_frames = config.get("processing", {}).get("frame_selection", {}).get("num_frames", 1)

    logging.info(f"Extracting {num_frames} frames from trajectories...")
    for domain_id, result in tqdm(results.items(), desc="Extracting frames"):
        if not result.get("pdb_saved", False):
            continue

        h5_path = os.path.join(input_dir, f"mdcath_dataset_{domain_id}.h5")
        if not os.path.exists(h5_path):
            logging.warning(f"H5 file not found for domain {domain_id}: {h5_path}")
            continue

        loader = H5DataLoader(h5_path, config)

        # For each temperature and replica
        for temp in temps:
            for r in range(num_replicas):
                replica = str(r)
                
                # Extract coordinates with RMSD and gyration data
                # Use special value -999 to request all frames if we need multiple
                frame_param = -999 if num_frames > 1 else -1
                coords_result = loader.extract_coordinates(temp, replica, frame=frame_param)
                
                if coords_result is not None:
                    coords, resids, resnames, rmsd_data, gyration_data = coords_result
                    extract_success = extract_frames(
                        coords, resids, resnames,
                        domain_id, output_dir, temp, replica, config,
                        rmsd_data, gyration_data
                    )
                    if extract_success:
                        if "frames" not in results[domain_id]:
                            results[domain_id]["frames"] = []
                        results[domain_id]["frames"].append((temp, replica))

    return results
===== FILE: src/mdcath/processing/features.py =====


#!/usr/bin/env python3
"""
Processing module for generating ML features.
"""

import os
import logging
import shutil  # <-- Added to fix "name 'shutil' is not defined" DSSP fallback error
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List, Tuple, Union
from tqdm import tqdm

from src.mdcath.processing.core_exterior import compute_core_exterior


def generate_ml_features(rmsf_data: Dict[str, pd.DataFrame],
                         core_exterior_data: Dict[str, pd.DataFrame],
                         dssp_data: Dict[str, Dict[str, pd.DataFrame]],
                         config: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
    """
    Generate ML features for all domains with improved handling of missing values.
    """
    try:
        # Get list of all domains
        domain_ids = set()
        for temp, df in rmsf_data.items():
            domain_ids.update(df["domain_id"].unique())

        domain_ids = list(domain_ids)
        logging.info(f"Generating ML features for {len(domain_ids)} domains")

        # Create feature dataframes for each temperature
        temps = [t for t in rmsf_data.keys() if t != "average"]
        feature_dfs = {}

        for temp in temps:
            # Start with RMSF data
            if temp not in rmsf_data:
                logging.warning(f"RMSF data not found for temperature {temp}")
                continue

            df = rmsf_data[temp].copy()

            # Ensure RMSF column is numeric
            rmsf_col = f"rmsf_{temp}"
            if rmsf_col in df.columns:
                df[rmsf_col] = pd.to_numeric(df[rmsf_col], errors='coerce').fillna(0.0)

            # Add protein size (number of residues for each domain)
            df["protein_size"] = df.groupby("domain_id")["resid"].transform("count")

            # Add normalized residue position
            df["normalized_resid"] = df.groupby("domain_id")["resid"].transform(
                lambda x: (x - x.min()) / max(x.max() - x.min(), 1)
            )

            # Create core_exterior column with default value
            if "core_exterior" not in df.columns:
                df["core_exterior"] = "core"  # Default to core (more conservative)
                
            # Create empty columns for missing data with default values
            if "relative_accessibility" not in df.columns:
                df["relative_accessibility"] = 0.5  # Default to moderate accessibility

            if "dssp" not in df.columns:
                df["dssp"] = "C"  # Default to coil

            # ---------------------------
            # MERGE CORE/EXTERIOR DATA
            # ---------------------------
            for domain_id in df["domain_id"].unique():
                if domain_id in core_exterior_data:
                    core_ext_df = core_exterior_data[domain_id]
                    
                    # Use regular merge to avoid dimension mismatch
                    domain_df = df[df["domain_id"] == domain_id][["domain_id", "resid"]].copy()
                    merged = pd.merge(domain_df, core_ext_df, on="resid", how="left")
                    
                    # Create mappings from resid to values
                    ce_mapping = dict(zip(merged["resid"], merged["core_exterior"]))
                    
                    # Apply the mapping to the original dataframe
                    domain_mask = df["domain_id"] == domain_id
                    df.loc[domain_mask, "core_exterior"] = df.loc[domain_mask, "resid"].map(ce_mapping).fillna("core")
                    
                    # If core_exterior_data contains relative_accessibility, use it
                    if "relative_accessibility" in core_ext_df.columns:
                        ra_mapping = dict(zip(core_ext_df["resid"], core_ext_df["relative_accessibility"]))
                        df.loc[domain_mask, "relative_accessibility"] = df.loc[domain_mask, "resid"].map(ra_mapping).fillna(0.5)

            # Ensure no missing values in core_exterior
            df["core_exterior"] = df["core_exterior"].fillna("core")

            # ---------------------------
            # ADD DSSP DATA
            # ---------------------------
            if temp in dssp_data:
                for replica, replica_dssp in dssp_data[temp].items():
                    if not replica_dssp.empty:
                        for domain_id in df["domain_id"].unique():
                            domain_dssp = replica_dssp[replica_dssp["domain_id"] == domain_id]
                            if not domain_dssp.empty:
                                # Convert resid to numeric carefully
                                domain_dssp.loc[:, "resid"] = pd.to_numeric(domain_dssp["resid"], errors='coerce')
                                
                                # Create mappings
                                dssp_mapping = dict(zip(domain_dssp["resid"], domain_dssp["dssp"]))
                                
                                # Apply mappings
                                domain_mask = df["domain_id"] == domain_id
                                df.loc[domain_mask, "dssp"] = df.loc[domain_mask, "resid"].map(dssp_mapping).fillna("C")
                                
                                # If relative_accessibility is present, use it
                                if "relative_accessibility" in domain_dssp.columns:
                                    ra_mapping = dict(zip(domain_dssp["resid"], domain_dssp["relative_accessibility"]))
                                    df.loc[domain_mask, "relative_accessibility"] = df.loc[domain_mask, "resid"].map(ra_mapping).fillna(0.5)
                                
                                # Break after first valid replica with data for this domain
                                break

            # Ensure no empty strings in DSSP
            df["dssp"] = df["dssp"].replace("", "C").replace(" ", "C").fillna("C")
            
            # Ensure relative_accessibility is numeric and not empty
            df["relative_accessibility"] = pd.to_numeric(df["relative_accessibility"], errors='coerce').fillna(0.5)

            # ---------------------------
            # ENCODE CATEGORICAL VARIABLES
            # ---------------------------
            # 1) Resname encoding: ensure all are strings
            if "resname" not in df.columns:
                # If resname is missing, create a placeholder
                df["resname"] = "UNK"

            # Convert to string and remove invalid placeholders
            df["resname"] = df["resname"].astype(str)
            filtered_resnames = [r for r in df["resname"].unique() if r not in ["nan", "None", ""]]
            unique_resnames = sorted(filtered_resnames)

            # Build mapping
            resname_mapping = {name: i+1 for i, name in enumerate(unique_resnames)}  # Start at 1
            df["resname_encoded"] = df["resname"].map(resname_mapping).fillna(0).astype(int)

            # 2) Core/Exterior encoding
            core_ext_mapping = {"core": 0, "exterior": 1, "unknown": 2}
            df["core_exterior_encoded"] = df["core_exterior"].map(core_ext_mapping).fillna(0).astype(int)

            # 3) DSSP encoding (3-state secondary structure)
            def encode_ss(ss):
                if ss in ["H", "G", "I"]:
                    return 0  # Helix
                elif ss in ["E", "B"]:
                    return 1  # Sheet
                else:
                    return 2  # Coil, Loop, or other

            df["secondary_structure_encoded"] = df["dssp"].apply(encode_ss)

            # Log DSSP encoding distribution
            dssp_codes = df["dssp"].value_counts().to_dict()
            encoded_values = df["secondary_structure_encoded"].value_counts().to_dict()
            logging.info(f"DSSP distribution for temp {temp}: {dssp_codes}")
            logging.info(f"Encoded SS distribution for temp {temp}: {encoded_values}")

            # Reorder columns to put domain_id first
            cols = df.columns.tolist()
            if "domain_id" in cols:
                cols.remove("domain_id")
                df = df[["domain_id"] + cols]

            # Final validation - ensure no NaN or empty values
            for col in df.columns:
                if df[col].dtype == 'object':
                    # For string columns, fill empty strings and NaNs with appropriate defaults
                    if col == 'dssp':
                        df[col] = df[col].replace('', 'C').replace(' ', 'C').fillna('C')
                    elif col == 'core_exterior':
                        df[col] = df[col].replace('', 'core').fillna('core')
                    elif col == 'resname':
                        df[col] = df[col].replace('', 'UNK').fillna('UNK')
                    else:
                        df[col] = df[col].fillna('unknown')
                else:
                    # For numeric columns, fill NaNs with appropriate defaults
                    if col == 'relative_accessibility':
                        df[col] = df[col].fillna(0.5)
                    else:
                        df[col] = df[col].fillna(0)

            # Store the feature dataframe
            feature_dfs[temp] = df

        # --------------------------------
        # CALCULATE AVERAGE FEATURES ACROSS TEMPS
        # --------------------------------
        if temps:
            avg_df = feature_dfs[temps[0]].copy(deep=True)

            # Calculate average RMSF across temperatures
            rmsf_cols = [f"rmsf_{temp}" for temp in temps]
            if all(col in avg_df.columns for col in rmsf_cols):
                avg_df["rmsf_average"] = avg_df[rmsf_cols].mean(axis=1)
            else:
                # If missing some temperature data
                available_cols = [col for col in rmsf_cols if col in avg_df.columns]
                if available_cols:
                    avg_df["rmsf_average"] = avg_df[available_cols].mean(axis=1)
                else:
                    # No RMSF data available
                    avg_df["rmsf_average"] = 0.0

            feature_dfs["average"] = avg_df

        return feature_dfs

    except Exception as e:
        logging.error(f"Failed to generate ML features: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return {}
    
def save_ml_features(feature_dfs: Dict[str, pd.DataFrame], output_dir: str) -> bool:
    """
    Save ML features to CSV files.

    Args:
        feature_dfs: Dictionary with ML feature dataframes
        output_dir: Directory to save CSV files

    Returns:
        Boolean indicating if saving was successful
    """
    try:
        os.makedirs(output_dir, exist_ok=True)

        for temp, df in feature_dfs.items():
            if temp == "average":
                output_file = os.path.join(output_dir, "final_dataset_temperature_average.csv")
            else:
                output_file = os.path.join(output_dir, f"final_dataset_temperature_{temp}.csv")

            df.to_csv(output_file, index=False)
            logging.info(f"Saved ML features to {output_file}")

        return True
    except Exception as e:
        logging.error(f"Failed to save ML features: {e}")
        return False


def process_ml_features(rmsf_results: Dict[str, Any],
                        pdb_results: Dict[str, Any],
                        domain_results: Dict[str, Dict[str, Any]],
                        config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process ML features for all domains.
    """
    output_dir = config.get("output", {}).get("base_dir", "./outputs")

    # Extract RMSF data
    replica_averages = rmsf_results.get("replica_averages", {})
    temperature_average = rmsf_results.get("temperature_average")

    if not replica_averages:
        logging.error("No RMSF data available for ML feature generation")
        return {"success": False, "error": "No RMSF data available"}

    # Create dictionary with all RMSF data
    rmsf_data = replica_averages.copy()
    if temperature_average is not None:
        rmsf_data["average"] = temperature_average

    # Compute core/exterior data
    core_exterior_data = {}
    logging.info("Computing core/exterior classification for domains")
    for domain_id, result in tqdm(pdb_results.items(), desc="Core/exterior classification"):
        if not result.get("pdb_saved", False):
            continue

        pdb_path = result.get("pdb_path")
        if not pdb_path or not os.path.exists(pdb_path):
            logging.warning(f"PDB file not found for domain {domain_id}")
            continue

        core_ext_df = compute_core_exterior(pdb_path, config)
        if core_ext_df is not None:
            core_exterior_data[domain_id] = core_ext_df

    # Collect DSSP data
    dssp_data = {}
    temps = [str(t) for t in config.get("temperatures", [320, 348, 379, 413, 450])]

    logging.info("Collecting DSSP data")
    for domain_id, result in tqdm(domain_results.items(), desc="Processing DSSP data"):
        if not result.get("success", False):
            continue

        domain_dssp = result.get("dssp_data", {})
        for temp in temps:
            if temp in domain_dssp:
                if temp not in dssp_data:
                    dssp_data[temp] = {}

                for replica, df in domain_dssp[temp].items():
                    if replica not in dssp_data[temp]:
                        dssp_data[temp][replica] = []
                    dssp_data[temp][replica].append(df)

    # Concatenate DSSP dataframes
    logging.info("Concatenating DSSP data")
    for temp in temps:
        if temp in dssp_data:
            for replica in dssp_data[temp]:
                if dssp_data[temp][replica]:
                    dssp_data[temp][replica] = pd.concat(dssp_data[temp][replica], ignore_index=True)

    # Generate ML features
    logging.info("Generating ML features")
    feature_dfs = generate_ml_features(rmsf_data, core_exterior_data, dssp_data, config)

    if not feature_dfs:
        logging.error("Failed to generate ML features")
        return {"success": False, "error": "Feature generation failed"}

    # Save ML features
    ml_dir = os.path.join(output_dir, "ML_features")
    save_success = save_ml_features(feature_dfs, ml_dir)

    return {
        "success": save_success,
        "feature_dfs": feature_dfs,
        "output_dir": ml_dir
    }

===== FILE: src/mdcath/processing/visualization.py =====
#!/usr/bin/env python3
"""
Module for generating visualizations of processed mdCATH data.
"""

import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Optional, List, Tuple

def create_temperature_summary_heatmap(rmsf_data: Dict[str, pd.DataFrame], 
                                     output_dir: str) -> Optional[str]:
    """
    Create a heatmap showing RMSF values across temperatures for all domains.
    
    Args:
        rmsf_data: Dictionary with RMSF data for all temperatures
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Extract temperature values
        temps = [temp for temp in rmsf_data.keys() if temp != "average"]
        
        if not temps:
            logging.warning("No temperature data available for heatmap")
            return None
            
        # Prepare data for heatmap
        domain_ids = set()
        for temp in temps:
            if temp in rmsf_data:
                domain_ids.update(rmsf_data[temp]["domain_id"].unique())
        
        domain_ids = sorted(list(domain_ids))
        
        # Create a dataframe for the heatmap
        heatmap_data = []
        
        for domain_id in domain_ids:
            domain_data = {"domain_id": domain_id}
            
            for temp in temps:
                if temp in rmsf_data:
                    domain_temp_data = rmsf_data[temp][rmsf_data[temp]["domain_id"] == domain_id]
                    if not domain_temp_data.empty:
                        domain_data[f"rmsf_{temp}"] = domain_temp_data[f"rmsf_{temp}"].mean()
            
            heatmap_data.append(domain_data)
        
        if not heatmap_data:
            logging.warning("No data available for heatmap")
            return None
            
        # Create dataframe and pivot for heatmap
        heatmap_df = pd.DataFrame(heatmap_data)
        heatmap_pivot = heatmap_df.set_index("domain_id")
        
        # Create heatmap
        plt.figure(figsize=(12, len(domain_ids) * 0.4 + 2))
        sns.heatmap(heatmap_pivot, annot=True, cmap="viridis", fmt=".3f")
        plt.title("Average RMSF by Domain and Temperature")
        plt.xlabel("Temperature (K)")
        plt.ylabel("Domain ID")
        plt.tight_layout()
        
        # Save figure
        output_path = os.path.join(vis_dir, "temperature_summary.png")
        plt.savefig(output_path, dpi=300)
        plt.close()
        
        logging.info(f"Temperature summary heatmap saved to {output_path}")
        return output_path
    except Exception as e:
        logging.error(f"Failed to create temperature summary heatmap: {e}")
        return None

def create_temperature_average_summary(temperature_average: pd.DataFrame, 
                                     output_dir: str) -> Optional[str]:
    """
    Create a visualization showing average RMSF across temperatures.
    
    Args:
        temperature_average: DataFrame with average RMSF values across all temperatures
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        if temperature_average is None or temperature_average.empty:
            logging.warning("No temperature average data available for summary")
            return None
            
        # Group by domain_id and calculate statistics
        domain_stats = temperature_average.groupby("domain_id")["rmsf_average"].agg(
            ["mean", "std", "min", "max"]).reset_index()
        
        # Sort by mean RMSF
        domain_stats = domain_stats.sort_values("mean", ascending=False)
        
        # Create bar plot
        plt.figure(figsize=(12, 8))
        plt.bar(domain_stats["domain_id"], domain_stats["mean"], yerr=domain_stats["std"])
        plt.xticks(rotation=90)
        plt.title("Average RMSF by Domain (Across All Temperatures)")
        plt.xlabel("Domain ID")
        plt.ylabel("Average RMSF (nm)")
        plt.tight_layout()
        
        # Save figure
        output_path = os.path.join(vis_dir, "temperature_average_summary.png")
        plt.savefig(output_path, dpi=300)
        plt.close()
        
        logging.info(f"Temperature average summary saved to {output_path}")
        return output_path
    except Exception as e:
        logging.error(f"Failed to create temperature average summary: {e}")
        return None

def create_rmsf_distribution_plots(rmsf_data: Dict[str, pd.DataFrame], 
                                  output_dir: str) -> Optional[str]:
    """
    Create distribution plots (violin plot and histogram) showing RMSF distribution by temperature.
    
    Args:
        rmsf_data: Dictionary with RMSF data for all temperatures
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Extract temperature values
        temps = [temp for temp in rmsf_data.keys() if temp != "average"]
        
        if not temps:
            logging.warning("No temperature data available for distribution plots")
            return None
            
        # Prepare data for plotting
        dist_data = []
        
        for temp in temps:
            if temp in rmsf_data:
                temp_df = rmsf_data[temp]
                rmsf_col = f"rmsf_{temp}"
                
                if rmsf_col in temp_df.columns:
                    for _, row in temp_df.iterrows():
                        dist_data.append({
                            "Temperature": temp,
                            "RMSF": row[rmsf_col]
                        })
        
        if not dist_data:
            logging.warning("No data available for distribution plots")
            return None
            
        # Create dataframe for plotting
        dist_df = pd.DataFrame(dist_data)
        
        # Create violin plot
        plt.figure(figsize=(10, 6))
        sns.violinplot(x="Temperature", y="RMSF", data=dist_df)
        plt.title("RMSF Distribution by Temperature")
        plt.xlabel("Temperature (K)")
        plt.ylabel("RMSF (nm)")
        plt.tight_layout()
        
        # Save violin plot
        violin_path = os.path.join(vis_dir, "rmsf_violin_plot.png")
        plt.savefig(violin_path, dpi=300)
        plt.close()
        
        # Create histogram
        plt.figure(figsize=(10, 6))
        for temp in temps:
            temp_data = dist_df[dist_df["Temperature"] == temp]["RMSF"]
            if not temp_data.empty:
                sns.histplot(temp_data, kde=True, label=f"{temp}K")
        
        plt.title("RMSF Histogram by Temperature")
        plt.xlabel("RMSF (nm)")
        plt.ylabel("Frequency")
        plt.legend()
        plt.tight_layout()
        
        # Save histogram
        hist_path = os.path.join(vis_dir, "rmsf_histogram.png")
        plt.savefig(hist_path, dpi=300)
        plt.close()
        
        logging.info(f"RMSF distribution plots saved to {violin_path} and {hist_path}")
        return violin_path
    except Exception as e:
        logging.error(f"Failed to create RMSF distribution plots: {e}")
        return None

def create_amino_acid_rmsf_plot(rmsf_data: Dict[str, pd.DataFrame], 
                              output_dir: str) -> Optional[str]:
    """
    Create a violin plot showing RMSF distribution by amino acid type.
    
    Args:
        rmsf_data: Dictionary with RMSF data for all temperatures
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Use temperature average if available
        if "average" in rmsf_data and not rmsf_data["average"].empty:
            aa_data = []
            
            avg_df = rmsf_data["average"]
            for _, row in avg_df.iterrows():
                aa_data.append({
                    "Residue": row["resname"],
                    "RMSF": row["rmsf_average"]
                })
                
            # Create dataframe for plotting
            aa_df = pd.DataFrame(aa_data)
            
            # Create violin plot
            plt.figure(figsize=(14, 8))
            sns.violinplot(x="Residue", y="RMSF", data=aa_df, order=sorted(aa_df["Residue"].unique()))
            plt.title("RMSF Distribution by Amino Acid Type")
            plt.xlabel("Amino Acid")
            plt.ylabel("RMSF (nm)")
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            # Save figure
            output_path = os.path.join(vis_dir, "amino_acid_rmsf_violin_plot.png")
            plt.savefig(output_path, dpi=300)
            plt.close()
            
            logging.info(f"Amino acid RMSF violin plot saved to {output_path}")
            return output_path
        else:
            logging.warning("No average temperature data available for amino acid plot")
            return None
    except Exception as e:
        logging.error(f"Failed to create amino acid RMSF plot: {e}")
        return None

def create_replica_variance_plot(rmsf_data: Dict[str, Dict[str, pd.DataFrame]],
                               output_dir: str) -> Optional[str]:
    """
    Create a plot showing variance of RMSF values across different replicas.
    
    Args:
        rmsf_data: Dictionary with RMSF data for all temperatures and replicas
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Extract temperatures
        temps = list(rmsf_data.keys())
        
        if not temps:
            logging.warning("No temperature data available for replica variance plot")
            return None
            
        # Calculate variance for each temperature
        variance_data = []
        
        for temp in temps:
            replicas = rmsf_data.get(temp, {})
            
            if replicas:
                # Get all domain_ids and resids
                domain_resids = set()
                
                for replica, df in replicas.items():
                    if df is not None and not df.empty:
                        for _, row in df.iterrows():
                            domain_resids.add((row["domain_id"], row["resid"]))
                
                # Calculate variance for each domain_id and resid
                for domain_id, resid in domain_resids:
                    rmsf_values = []
                    
                    for replica, df in replicas.items():
                        if df is not None and not df.empty:
                            mask = (df["domain_id"] == domain_id) & (df["resid"] == resid)
                            if mask.any():
                                rmsf_values.append(df.loc[mask, f"rmsf_{temp}"].values[0])
                    
                    if len(rmsf_values) > 1:
                        variance_data.append({
                            "Temperature": temp,
                            "Domain": domain_id,
                            "Resid": resid,
                            "Variance": np.var(rmsf_values)
                        })
        
        if not variance_data:
            logging.warning("No data available for replica variance plot")
            return None
            
        # Create dataframe for plotting
        variance_df = pd.DataFrame(variance_data)
        
        # Create box plot
        plt.figure(figsize=(10, 6))
        sns.boxplot(x="Temperature", y="Variance", data=variance_df)
        plt.title("RMSF Variance Across Replicas")
        plt.xlabel("Temperature (K)")
        plt.ylabel("Variance of RMSF (nm²)")
        plt.tight_layout()
        
        # Save figure
        output_path = os.path.join(vis_dir, "replica_variance_plot.png")
        plt.savefig(output_path, dpi=300)
        plt.close()
        
        logging.info(f"Replica variance plot saved to {output_path}")
        return output_path
    except Exception as e:
        logging.error(f"Failed to create replica variance plot: {e}")
        return None

def create_dssp_rmsf_correlation_plot(feature_dfs: Dict[str, pd.DataFrame],
                                    output_dir: str) -> Optional[str]:
    """
    Create a visualization showing the relationship between secondary structure and RMSF values.
    
    Args:
        feature_dfs: Dictionary with ML feature dataframes
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Use average temperature data if available
        if "average" in feature_dfs and not feature_dfs["average"].empty:
            avg_df = feature_dfs["average"]
            
            if "dssp" in avg_df.columns and "rmsf_average" in avg_df.columns:
                # Group by DSSP code and calculate statistics
                dssp_stats = avg_df.groupby("dssp")["rmsf_average"].agg(
                    ["mean", "std", "count"]).reset_index()
                
                # Sort by count (to prioritize common secondary structures)
                dssp_stats = dssp_stats.sort_values("count", ascending=False)
                
                # Create bar plot
                plt.figure(figsize=(12, 8))
                plt.bar(dssp_stats["dssp"], dssp_stats["mean"], yerr=dssp_stats["std"])
                
                # Add count as text on each bar
                for i, row in dssp_stats.iterrows():
                    plt.text(i, row["mean"] + row["std"] + 0.01, 
                            f"n={int(row['count'])}", 
                            ha='center', va='bottom', rotation=0)
                
                plt.title("Average RMSF by Secondary Structure (DSSP)")
                plt.xlabel("DSSP Code")
                plt.ylabel("Average RMSF (nm)")
                plt.tight_layout()
                
                # Save figure
                output_path = os.path.join(vis_dir, "dssp_rmsf_correlation_plot.png")
                plt.savefig(output_path, dpi=300)
                plt.close()
                
                logging.info(f"DSSP vs RMSF correlation plot saved to {output_path}")
                return output_path
            else:
                logging.warning("DSSP or RMSF data not found in feature dataframe")
                return None
        else:
            logging.warning("No average temperature data available for DSSP correlation plot")
            return None
    except Exception as e:
        logging.error(f"Failed to create DSSP vs RMSF correlation plot: {e}")
        return None

def create_feature_correlation_plot(feature_dfs: Dict[str, pd.DataFrame],
                                  output_dir: str) -> Optional[str]:
    """
    Create a visualization highlighting relationships between structural features and RMSF.
    
    Args:
        feature_dfs: Dictionary with ML feature dataframes
        output_dir: Directory to save visualization
        
    Returns:
        Path to the saved figure or None if creation fails
    """
    try:
        # Ensure output directory exists
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Use average temperature data if available
        if "average" in feature_dfs and not feature_dfs["average"].empty:
            avg_df = feature_dfs["average"]
            
            # Select numerical columns for correlation
            numerical_cols = []
            for col in avg_df.columns:
                if col.startswith("rmsf_") or col == "normalized_resid" or col.endswith("_encoded"):
                    numerical_cols.append(col)
            
            if not numerical_cols:
                logging.warning("No numerical feature columns found for correlation plot")
                return None
                
            # Calculate correlation
            corr_df = avg_df[numerical_cols].corr()
            
            # Create heatmap
            plt.figure(figsize=(10, 8))
            sns.heatmap(corr_df, annot=True, cmap="coolwarm", fmt=".2f", 
                       vmin=-1, vmax=1, center=0)
            plt.title("Correlation Between Features and RMSF")
            plt.tight_layout()
            
            # Save figure
            output_path = os.path.join(vis_dir, "feature_correlation_plot.png")
            plt.savefig(output_path, dpi=300)
            plt.close()
            
            logging.info(f"Feature correlation plot saved to {output_path}")
            return output_path
        else:
            logging.warning("No average temperature data available for feature correlation plot")
            return None
    except Exception as e:
        logging.error(f"Failed to create feature correlation plot: {e}")
        return None

def generate_visualizations(rmsf_results: Dict[str, Any], 
                          ml_results: Dict[str, Any],
                          domain_results: Dict[str, Dict[str, Any]],
                          config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate all required visualizations.
    
    Args:
        rmsf_results: Dictionary with RMSF processing results
        ml_results: Dictionary with ML feature processing results
        domain_results: Dictionary with processing results for all domains
        config: Configuration dictionary
        
    Returns:
        Dictionary with visualization results
    """
    output_dir = config.get("output", {}).get("base_dir", "./outputs")
    
    # Extract required data
    replica_averages = rmsf_results.get("replica_averages", {})
    temperature_average = rmsf_results.get("temperature_average")
    combined_rmsf_data = rmsf_results.get("combined_rmsf_data", {})
    feature_dfs = ml_results.get("feature_dfs", {})
    
    # Generate visualizations
    results = {}
    
    # Temperature summary heatmap
    results["temperature_summary"] = create_temperature_summary_heatmap(
        replica_averages, output_dir)
        
    # Temperature average summary
    results["temperature_average_summary"] = create_temperature_average_summary(
        temperature_average, output_dir)
        
    # RMSF distribution plots
    results["rmsf_distribution"] = create_rmsf_distribution_plots(
        replica_averages, output_dir)
        
    # Amino acid RMSF plot
    results["amino_acid_rmsf"] = create_amino_acid_rmsf_plot(
        {"average": temperature_average}, output_dir)
        
    # Replica variance plot
    results["replica_variance"] = create_replica_variance_plot(
        combined_rmsf_data, output_dir)
        
    # DSSP vs RMSF correlation plot
    results["dssp_rmsf_correlation"] = create_dssp_rmsf_correlation_plot(
        feature_dfs, output_dir)
        
    # Feature correlation plot
    results["feature_correlation"] = create_feature_correlation_plot(
        feature_dfs, output_dir)
    
    return results


=======================================
Extracting First 10 Lines from Data Directory (Ignoring Binary Files)
=======================================

Data directory /home/s_felix/mdcath-processor/data6 does not exist.
Data Directory: /home/s_felix/mdcath-processor

Folder Structure in Data Directory:
.
├── AI_context.sh
├── AI_context.txt
├── check_environment.py
├── LICENSE
├── main.py
├── mdcath_processing.log
├── msms_executables
│   ├── 1crn.pdb
│   ├── 1crn.xyzr
│   ├── 1crn.xyzrn
│   ├── atmtypenumbers
│   ├── msms.1
│   ├── msms.html
│   ├── msms_i86_64Linux2_2.6.1.tar.gz
│   ├── msms.x86_64Linux2.2.6.1
│   ├── msms.x86_64Linux2.2.6.1.staticgcc
│   ├── pdb_to_xyzr
│   ├── pdb_to_xyzrn
│   ├── README
│   ├── ReleaseNotes
│   ├── test.pdb
│   └── test.xyzr
├── outputs
├── README.md
├── requirements.txt
├── setup.py
├── setup.sh
├── src
│   ├── mdcath
│   │   ├── config
│   │   │   ├── default_config.yaml
│   │   │   └── __init__.py
│   │   ├── core
│   │   │   ├── data_loader.py
│   │   │   ├── __init__.py
│   │   │   └── __pycache__
│   │   │       ├── data_loader.cpython-39.pyc
│   │   │       └── __init__.cpython-39.pyc
│   │   ├── __init__.py
│   │   ├── processing
│   │   │   ├── core_exterior.py
│   │   │   ├── features.py
│   │   │   ├── __init__.py
│   │   │   ├── pdb.py
│   │   │   ├── __pycache__
│   │   │   │   ├── core_exterior.cpython-39.pyc
│   │   │   │   ├── features.cpython-39.pyc
│   │   │   │   ├── __init__.cpython-39.pyc
│   │   │   │   ├── pdb.cpython-39.pyc
│   │   │   │   ├── rmsf.cpython-39.pyc
│   │   │   │   ├── visualization.cpython-39.pyc
│   │   │   │   └── voxelizer.cpython-39.pyc
│   │   │   ├── rmsf.py
│   │   │   ├── visualization.py
│   │   │   └── voxelizer.py
│   │   └── __pycache__
│   │       └── __init__.cpython-39.pyc
│   └── mdcath.egg-info
│       ├── dependency_links.txt
│       ├── PKG-INFO
│       ├── requires.txt
│       ├── SOURCES.txt
│       └── top_level.txt
└── test_h5_loading.py

11 directories, 53 files

Extracting First 10 Lines from Each File in Data Directory (Excluding Binary & pipeline.log):
-------------------------------------------------------------------------------------
===== FILE: ./msms_executables/pdb_to_xyzrn =====
#! /bin/sh
# extract x,y,z from PDB file, generate radius of each atom
# Hydrogens are presumed to be missing ("united atom" approach) unless -h given
# later: options for alternate radius and pattern files
# --- Mike Pique, The Scripps Research Institute
#
# input: pdb file as argument or stdin
# output: new xyzr file to stdout
#
# Options:

===== FILE: ./msms_executables/pdb_to_xyzr =====
#! /bin/sh
# extract x,y,z from PDB file, generate radius of each atom
# Hydrogens are presumed to be missing ("united atom" approach) unless -h given
# later: options for alternate radius and pattern files
# --- Mike Pique, The Scripps Research Institute
#
# input: pdb file as argument or stdin
# output: new xyzr file to stdout
#
# Options:

===== FILE: ./msms_executables/test.xyzr =====
  -3.917    0.791   -0.042 1.70
  -2.616    0.112   -0.179 2.00
  -1.490    1.102   -0.001 1.74
  -1.699    2.154    0.594 1.40
  -2.440   -0.950   -1.300 2.00
  -2.544   -0.372    0.751 1.20
  -3.250   -1.657   -1.300 1.20

===== FILE: ./msms_executables/test.pdb =====
ATOM      1  N   ALA A   1      -3.917   0.791  -0.042  1.00  0.00           N  
ATOM      2  CA  ALA A   1      -2.616   0.112  -0.179  1.00  0.00           C  
ATOM      3  C   ALA A   1      -1.490   1.102  -0.001  1.00  0.00           C  
ATOM      4  O   ALA A   1      -1.699   2.154   0.594  1.00  0.00           O  
ATOM      5  CB  ALA A   1      -2.440  -0.950  -1.300  1.00  0.00           C  
ATOM      6  HA  ALA A   1      -2.544  -0.372   0.751  1.00  0.00           H  
ATOM      7  HB1 ALA A   1      -3.250  -1.657  -1.300  1.00  0.00           H  
TER
END

===== FILE: ./msms_executables/msms.html =====
<html>
<head>
<title>
MSMS man page
</title>
</head>

<body>
<pre>
<H3>MSMS(1)                   User Commands                   MSMS(1)</H3>

===== FILE: ./msms_executables/atmtypenumbers =====
# atmtypenumber 
# $Revision: 1.13 $
# Maps residue type and atom name pairs into Connolly ".atm" numeric codes
#  as used in MS and AMS, and into actual radius values
#
# Format: (blank lines and lines beginning with # are ignored)
#  Any number of blanks or tabs separate fields.
#
# Format of "radius" entries
# Field 1: keyword "radius"

===== FILE: ./msms_executables/ReleaseNotes =====
Version 2.6.1
- modified find_free_edges to continue when an atom is found to
  be completely inside another atom

Version 2.6.0
- Dual density support:
  It is now possible to compute a molecular surface with 2 densities of 
  vertices.
  The command line accepts a new keyword "hdensity" for specifying the vertex 
  density to be used in the high density region.

===== FILE: ./msms_executables/1crn.xyzr =====
  17.047   14.099    3.625 1.70
  16.967   12.784    4.338 2.00
  15.685   12.755    5.133 1.74
  15.268   13.825    5.594 1.40
  18.170   12.703    5.337 2.00
  19.334   12.829    4.463 1.60
  18.150   11.546    6.304 2.00
  15.115   11.555    5.265 1.70
  13.856   11.469    6.066 2.00
  14.164   10.785    7.379 1.74

===== FILE: ./msms_executables/1crn.xyzrn =====
17.047000 14.099000 3.625000 1.700000 1 N_THR_1
16.967000 12.784000 4.338000 2.000000 1 CA_THR_1
15.685000 12.755000 5.133000 1.740000 1 C_THR_1
15.268000 13.825000 5.594000 1.400000 1 O_THR_1
18.170000 12.703000 5.337000 2.000000 1 CB_THR_1
19.334000 12.829000 4.463000 1.600000 1 OG1_THR_1
18.150000 11.546000 6.304000 2.000000 1 CG2_THR_1
15.115000 11.555000 5.265000 1.700000 1 N_THR_2
13.856000 11.469000 6.066000 2.000000 1 CA_THR_2
14.164000 10.785000 7.379000 1.740000 1 C_THR_2

===== FILE: ./msms_executables/msms.1 =====
.\" @(#)nroff.1 1.33 90/02/15 SMI;
.TH MSMS 1 v2.5 "Nov. 3 1999"
.SH NAME
msms 
.SH SYNOPSIS
.B msms
[
.BI \-if \ filename
] [
.BI \-of \ filename

===== FILE: ./msms_executables/README =====
MSMS v2.5.7 (Michel F. SANNER Jan. 2006)

MSMS computes, for a given probe radius, the reduced surface of a set of
spheres. An analytical description of the solvent excluded surface is
computed from the reduced surface. Special attention is paid to the proper
handling of self-intersecting parts of the surface called singularities.
This analytical model of the solvent excluded surface can be triangulated
with a user specified vertex density (see references for more information).

############################################################################

===== FILE: ./AI_context.txt =====
Working Directory: /home/s_felix/mdcath-processor

File Structure:
.
├── AI_context.sh
├── AI_context.txt
├── check_environment.py
├── LICENSE
├── main.py
├── mdcath_processing.log

===== FILE: ./setup.sh =====
# #!/bin/bash

# # Make the script exit on error
# set -e

# echo "Generating code files for mdCATH project..."

# # ------------------------------------
# # INIT FILES
# # ------------------------------------

===== FILE: ./AI_context.sh =====
#!/bin/bash

# Define output file (adjusted to the current project structure)
OUTPUT_FILE="/home/s_felix/mdcath-processor/AI_context.txt"

# Start writing to output file
{
    echo "Working Directory: $(pwd)"
    echo ""
    echo "File Structure:"

===== FILE: ./setup.py =====
#!/usr/bin/env python3
"""
Setup script for mdCATH
"""

from setuptools import setup, find_packages

setup(
    name="mdcath",
    version="0.1.0",

===== FILE: ./mdcath_processing.log =====
2025-03-19 22:50:56,352 - h5py._conv - DEBUG - Creating converter from 3 to 5
2025-03-19 22:50:56,659 - h5py._conv - DEBUG - Creating converter from 3 to 5
2025-03-19 23:10:27,327 - h5py._conv - DEBUG - Creating converter from 3 to 5
2025-03-19 23:10:27,634 - h5py._conv - DEBUG - Creating converter from 3 to 5
2025-03-19 23:23:12,817 - h5py._conv - DEBUG - Creating converter from 3 to 5
2025-03-19 23:23:13,126 - h5py._conv - DEBUG - Creating converter from 3 to 5
2025-03-19 23:23:20,300 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-03-19 23:23:20,302 - matplotlib.font_manager - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.
2025-03-19 23:23:20,302 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/home/s_felix/.conda/envs/apo_env/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2025-03-19 23:23:20,302 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/home/s_felix/.conda/envs/apo_env/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05

===== FILE: ./.git/config =====
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[remote "origin"]
	url = https://github.com/Felixburton7/mdcath-processor2.git
	fetch = +refs/heads/*:refs/remotes/origin/*
[branch "main"]
	remote = origin

===== FILE: ./.git/description =====
Unnamed repository; edit this file 'description' to name the repository.

===== FILE: ./.git/HEAD =====
ref: refs/heads/main

===== FILE: ./.git/refs/remotes/origin/main =====
7929c8518d63c10d4b01bec71a9a509251e89544

===== FILE: ./.git/refs/heads/main =====
ed288409e8faed2d92b867370f91e3c81e91a9fb

===== FILE: ./.git/info/exclude =====
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

===== FILE: ./.git/COMMIT_EDITMSG =====
updated pipeline with sampel output

===== FILE: ./.git/hooks/pre-applypatch.sample =====
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-applypatch".


===== FILE: ./.git/hooks/pre-commit.sample =====
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git commit" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-commit".

if git rev-parse --verify HEAD >/dev/null 2>&1

===== FILE: ./.git/hooks/fsmonitor-watchman.sample =====
#!/usr/bin/perl

use strict;
use warnings;
use IPC::Open2;

# An example hook script to integrate Watchman
# (https://facebook.github.io/watchman/) with git to speed up detecting
# new and modified files.
#

===== FILE: ./.git/hooks/applypatch-msg.sample =====
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to "applypatch-msg".

===== FILE: ./.git/hooks/push-to-checkout.sample =====
#!/bin/sh

# An example hook script to update a checked-out tree on a git push.
#
# This hook is invoked by git-receive-pack(1) when it reacts to git
# push and updates reference(s) in its repository, and when the push
# tries to update the branch that is currently checked out and the
# receive.denyCurrentBranch configuration variable is set to
# updateInstead.
#

===== FILE: ./.git/hooks/pre-receive.sample =====
#!/bin/sh
#
# An example hook script to make use of push options.
# The example simply echoes all push options that start with 'echoback='
# and rejects all pushes when the "reject" push option is used.
#
# To enable this hook, rename this file to "pre-receive".

if test -n "$GIT_PUSH_OPTION_COUNT"
then

===== FILE: ./.git/hooks/update.sample =====
#!/bin/sh
#
# An example hook script to block unannotated tags from entering.
# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to "update".
#
# Config
# ------
# hooks.allowunannotated

===== FILE: ./.git/hooks/pre-merge-commit.sample =====
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git merge" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message to
# stderr if it wants to stop the merge commit.
#
# To enable this hook, rename this file to "pre-merge-commit".

. git-sh-setup

===== FILE: ./.git/hooks/pre-push.sample =====
#!/bin/sh

# An example hook script to verify what is about to be pushed.  Called by "git
# push" after it has checked the remote status, but before anything has been
# pushed.  If this script exits with a non-zero status nothing will be pushed.
#
# This hook is called with the following parameters:
#
# $1 -- Name of the remote to which the push is being done
# $2 -- URL to which the push is being done

===== FILE: ./.git/hooks/post-update.sample =====
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to "post-update".

exec git update-server-info

===== FILE: ./.git/hooks/commit-msg.sample =====
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by "git commit" with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to "commit-msg".


===== FILE: ./.git/hooks/pre-rebase.sample =====
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The "pre-rebase" hook is run just before "git rebase" starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#

===== FILE: ./.git/hooks/prepare-commit-msg.sample =====
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by "git commit" with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to "prepare-commit-msg".

===== FILE: ./.git/logs/HEAD =====
0000000000000000000000000000000000000000 e5d234b3768d56a714b360b82486d7dcf1c1daf9 Felix Burton <felixburton2002@gmail.com> 1742428799 +0000	commit (initial): first commit
e5d234b3768d56a714b360b82486d7dcf1c1daf9 0000000000000000000000000000000000000000 Felix Burton <felixburton2002@gmail.com> 1742428799 +0000	Branch: renamed refs/heads/master to refs/heads/main
0000000000000000000000000000000000000000 e5d234b3768d56a714b360b82486d7dcf1c1daf9 Felix Burton <felixburton2002@gmail.com> 1742428799 +0000	Branch: renamed refs/heads/master to refs/heads/main
e5d234b3768d56a714b360b82486d7dcf1c1daf9 e7829511d2beaccc14b25cd651bbef6837f0ef10 Felix Burton <felixburton2002@gmail.com> 1742428809 +0000	commit: initial commit
e7829511d2beaccc14b25cd651bbef6837f0ef10 16dc0354d39edebbe7342eed9cad9d8e1e2b5ca7 Felix Burton <felixburton2002@gmail.com> 1742603805 +0000	commit: fixed bugs
16dc0354d39edebbe7342eed9cad9d8e1e2b5ca7 7929c8518d63c10d4b01bec71a9a509251e89544 Felix Burton <felixburton2002@gmail.com> 1742604292 +0000	commit: readme
7929c8518d63c10d4b01bec71a9a509251e89544 ed288409e8faed2d92b867370f91e3c81e91a9fb Felix Burton <felixburton2002@gmail.com> 1742648708 +0000	commit: updated pipeline with sampel output

===== FILE: ./.git/logs/refs/remotes/origin/main =====
0000000000000000000000000000000000000000 e5d234b3768d56a714b360b82486d7dcf1c1daf9 Felix Burton <felixburton2002@gmail.com> 1742428801 +0000	update by push
e5d234b3768d56a714b360b82486d7dcf1c1daf9 e7829511d2beaccc14b25cd651bbef6837f0ef10 Felix Burton <felixburton2002@gmail.com> 1742428813 +0000	update by push
e7829511d2beaccc14b25cd651bbef6837f0ef10 16dc0354d39edebbe7342eed9cad9d8e1e2b5ca7 Felix Burton <felixburton2002@gmail.com> 1742603810 +0000	update by push
16dc0354d39edebbe7342eed9cad9d8e1e2b5ca7 7929c8518d63c10d4b01bec71a9a509251e89544 Felix Burton <felixburton2002@gmail.com> 1742604294 +0000	update by push

===== FILE: ./.git/logs/refs/heads/main =====
0000000000000000000000000000000000000000 e5d234b3768d56a714b360b82486d7dcf1c1daf9 Felix Burton <felixburton2002@gmail.com> 1742428799 +0000	commit (initial): first commit
e5d234b3768d56a714b360b82486d7dcf1c1daf9 e5d234b3768d56a714b360b82486d7dcf1c1daf9 Felix Burton <felixburton2002@gmail.com> 1742428799 +0000	Branch: renamed refs/heads/master to refs/heads/main
e5d234b3768d56a714b360b82486d7dcf1c1daf9 e7829511d2beaccc14b25cd651bbef6837f0ef10 Felix Burton <felixburton2002@gmail.com> 1742428809 +0000	commit: initial commit
e7829511d2beaccc14b25cd651bbef6837f0ef10 16dc0354d39edebbe7342eed9cad9d8e1e2b5ca7 Felix Burton <felixburton2002@gmail.com> 1742603805 +0000	commit: fixed bugs
16dc0354d39edebbe7342eed9cad9d8e1e2b5ca7 7929c8518d63c10d4b01bec71a9a509251e89544 Felix Burton <felixburton2002@gmail.com> 1742604292 +0000	commit: readme
7929c8518d63c10d4b01bec71a9a509251e89544 ed288409e8faed2d92b867370f91e3c81e91a9fb Felix Burton <felixburton2002@gmail.com> 1742648708 +0000	commit: updated pipeline with sampel output

===== FILE: ./LICENSE =====
MIT License

Copyright (c) 2025 mdCATH Processing Project

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

===== FILE: ./src/mdcath.egg-info/SOURCES.txt =====
LICENSE
README.md
setup.py
src/mdcath/__init__.py
src/mdcath.egg-info/PKG-INFO
src/mdcath.egg-info/SOURCES.txt
src/mdcath.egg-info/dependency_links.txt
src/mdcath.egg-info/requires.txt
src/mdcath.egg-info/top_level.txt
src/mdcath/config/__init__.py

===== FILE: ./src/mdcath.egg-info/top_level.txt =====
mdcath

===== FILE: ./src/mdcath.egg-info/requires.txt =====
h5py
numpy
pandas
biopython
pyyaml
matplotlib
seaborn
tqdm

===== FILE: ./src/mdcath.egg-info/PKG-INFO =====
Metadata-Version: 2.2
Name: mdcath
Version: 0.1.0
Summary: Process mdCATH dataset for ML applications
Author: Biochemistry Team
Author-email: info@example.com
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3

===== FILE: ./src/mdcath/processing/core_exterior.py =====
#!/usr/bin/env python3
"""
Processing module for core/exterior classification.
"""

import os
import logging
import subprocess
import tempfile
import pandas as pd

===== FILE: ./src/mdcath/processing/voxelizer.py =====
#!/usr/bin/env python3
"""
Processing module for voxelizing protein structures using aposteriori.
"""

import os
import logging
import subprocess
import traceback
import sys

===== FILE: ./src/mdcath/processing/__init__.py =====
"""
Data processing modules for mdCATH
"""

===== FILE: ./src/mdcath/processing/rmsf.py =====
#!/usr/bin/env python3
"""
Processing module for RMSF data extraction and averaging.
"""

import os
import logging
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Any, Union

===== FILE: ./src/mdcath/processing/pdb.py =====
#!/usr/bin/env python3
"""
Processing module for PDB data extraction and cleaning.
"""

import os
import logging
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed

===== FILE: ./src/mdcath/processing/features.py =====


#!/usr/bin/env python3
"""
Processing module for generating ML features.
"""

import os
import logging
import shutil  # <-- Added to fix "name 'shutil' is not defined" DSSP fallback error

===== FILE: ./src/mdcath/processing/visualization.py =====
#!/usr/bin/env python3
"""
Module for generating visualizations of processed mdCATH data.
"""

import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

===== FILE: ./src/mdcath/config/default_config.yaml =====
input:
  mdcath_folder: "/mnt/datasets/MD_CATH/data"  # Path to the mdCATH folder
  domain_ids: [
    "12asA00",
    "153lA00",
    "16pkA02",
    "1a02F00",
    "1a05A00",
    "1a0aA00",
    "1a0hA01",

===== FILE: ./src/mdcath/config/__init__.py =====
"""
Configuration handling for mdCATH
"""

===== FILE: ./src/mdcath/__init__.py =====
"""
mdCATH - A package for processing mdCATH dataset for ML applications
"""

__version__ = '0.1.0'

===== FILE: ./src/mdcath/core/data_loader.py =====


#!/usr/bin/env python3
"""
Core functionality for loading and processing H5 data from mdCATH dataset.
"""

import os
import h5py
import logging

===== FILE: ./src/mdcath/core/__init__.py =====
"""
Core data loading and processing functions
"""

===== FILE: ./check_environment.py =====
#!/usr/bin/env python3
"""
Check environment setup for mdCATH processing.
"""

import os
import sys
import subprocess
import shutil
from colorama import init, Fore, Style

===== FILE: ./test_h5_loading.py =====
#!/usr/bin/env python3
"""
inspect_hdf5.py

Enhanced script to inspect and compare the contents of mdCATH .h5 files.
"""

import os
import sys
import h5py

===== FILE: ./README.md =====
# 🧪 mdCATH Dataset Processor

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![BioMolecular Analysis](https://img.shields.io/badge/Biomolecular-Analysis-green.svg)](https://github.com/yourusername/mdcath-processor)
[![Protein Dynamics](https://img.shields.io/badge/Protein-Dynamics-red.svg)](https://github.com/yourusername/mdcath-processor)

A comprehensive suite for processing mdCATH protein dynamics dataset to facilitate machine learning-based prediction of Root Mean Square Fluctuation (RMSF) from protein structures.

---

===== FILE: ./requirements.txt =====
h5py>=3.1.0
numpy>=1.19.0
pandas>=1.1.0
biopython>=1.78
pyyaml>=5.4.0
matplotlib>=3.3.0
seaborn>=0.11.0
tqdm>=4.50.0
pdbUtils
# Install aposteriori separately via pip

===== FILE: ./main.py =====
#!/usr/bin/env python3
"""
Main entry point for mdCATH dataset processing.
"""

import os
import sys
import logging
import argparse
import yaml

